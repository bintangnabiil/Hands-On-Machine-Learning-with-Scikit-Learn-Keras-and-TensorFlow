{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPFwOHaGFKrfujL/rZN8bJU",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bintangnabiil/Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow/blob/main/Rangkuman_Chapter_15.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Chapter 15: Processing Sequences Using RNNs and CNNs\n",
        "Chapter ini membahas tentang pemrosesan data sekuensial menggunakan Recurrent Neural Networks (RNNs) dan Convolutional Neural Networks (CNNs). Data sekuensial adalah data yang memiliki urutan temporal seperti time series, teks, audio, dan video.\n",
        "\n",
        "##1. Recurrent Neural Networks (RNNs)\n",
        "###Konsep Dasar RNN\n",
        "RNN adalah jenis neural network yang dirancang khusus untuk memproses data sekuensial. Berbeda dengan feedforward neural networks yang memproses input secara independen, RNN memiliki \"memori\" yang memungkinkannya mengingat informasi dari langkah waktu sebelumnya.\n",
        "\n",
        "###Karakteristik utama RNN:\n",
        "\n",
        "Memiliki koneksi rekuren yang membentuk loop\n",
        "Dapat memproses sekuens dengan panjang yang bervariasi\n",
        "Berbagi parameter antar langkah waktu\n",
        "Output pada waktu t bergantung pada input saat ini dan state tersembunyi dari waktu sebelumnya\n",
        "\n",
        "###Arsitektur RNN\n",
        "RNN memiliki dua input utama:\n",
        "\n",
        "- Input sequence (x_t): data pada waktu t\n",
        "- Hidden state (h_t-1): informasi dari langkah waktu sebelumnya\n",
        "\n",
        "Formula matematika RNN:\n",
        "$$\n",
        "h_t = tanh(W_xh * x_t + W_hh * h_t-1 + b_h)\n",
        "$$\n",
        "\n",
        "$$\n",
        "y_t = W_hy * h_t + b_y\n",
        "$$\n"
      ],
      "metadata": {
        "id": "IwpREpgC_3lX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Implementasi RNN Sederhana"
      ],
      "metadata": {
        "id": "6kU-3aoAAqHz"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LQzlsxOf_edK",
        "outputId": "a173cdd4-6a74-42ce-82b5-b3fff187c9b4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training data shape: (7000, 50, 1)\n",
            "Training labels shape: (7000, 1)\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Membuat data time series sederhana untuk demonstrasi\n",
        "def generate_time_series(batch_size, n_steps):\n",
        "    freq1, freq2, offsets1, offsets2 = np.random.rand(4, batch_size, 1)\n",
        "    time = np.linspace(0, 1, n_steps)\n",
        "    series = 0.5 * np.sin((time - offsets1) * (freq1 * 10 + 10))\n",
        "    series += 0.2 * np.sin((time - offsets2) * (freq2 * 20 + 20))\n",
        "    series += 0.1 * (np.random.rand(batch_size, n_steps) - 0.5)\n",
        "    return series[..., np.newaxis].astype(np.float32)\n",
        "\n",
        "# Generate training data\n",
        "n_steps = 50\n",
        "series = generate_time_series(10000, n_steps + 1)\n",
        "X_train, y_train = series[:7000, :n_steps], series[:7000, -1]\n",
        "X_valid, y_valid = series[7000:9000, :n_steps], series[7000:9000, -1]\n",
        "X_test, y_test = series[9000:, :n_steps], series[9000:, -1]\n",
        "\n",
        "print(f\"Training data shape: {X_train.shape}\")\n",
        "print(f\"Training labels shape: {y_train.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Simple RNN Model"
      ],
      "metadata": {
        "id": "LUyHfK_2A26I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Model RNN sederhana untuk prediksi time series\n",
        "model = keras.models.Sequential([\n",
        "    keras.layers.SimpleRNN(20, return_sequences=True, input_shape=[None, 1]),\n",
        "    keras.layers.SimpleRNN(20, return_sequences=True),\n",
        "    keras.layers.SimpleRNN(20),\n",
        "    keras.layers.Dense(1)\n",
        "])\n",
        "\n",
        "model.compile(optimizer='adam', loss='mse')\n",
        "model.summary()\n",
        "\n",
        "# Training model\n",
        "history = model.fit(X_train, y_train, epochs=20,\n",
        "                    validation_data=(X_valid, y_valid),\n",
        "                    verbose=1)"
      ],
      "metadata": {
        "id": "ZnurztIsA4_0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##2. Masalah Vanishing Gradient pada RNN\n",
        "###Penjelasan Masalah\n",
        "RNN standar mengalami masalah vanishing gradient ketika memproses sekuens yang panjang. Ini terjadi karena:\n",
        "\n",
        "- Gradient yang dipropagasi mundur menjadi semakin kecil\n",
        "- Informasi dari langkah waktu yang jauh menjadi hilang\n",
        "- Model sulit mempelajari dependensi jangka panjang\n",
        "<br><br>\n",
        "\n",
        "###Solusi: LSTM dan GRU\n",
        "###Long Short-Term Memory (LSTM)\n",
        "LSTM mengatasi masalah vanishing gradient dengan menggunakan:\n",
        "\n",
        "- Cell state: jalur informasi jangka panjang\n",
        "- Forget gate: menentukan informasi mana yang akan dilupakan\n",
        "- Input gate: menentukan informasi baru yang akan disimpan\n",
        "- Output gate: menentukan bagian mana dari cell state yang akan dioutput"
      ],
      "metadata": {
        "id": "lxkXXWw-BN7q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# LSTM Model\n",
        "lstm_model = keras.models.Sequential([\n",
        "    keras.layers.LSTM(20, return_sequences=True, input_shape=[None, 1]),\n",
        "    keras.layers.LSTM(20, return_sequences=True),\n",
        "    keras.layers.LSTM(20),\n",
        "    keras.layers.Dense(1)\n",
        "])\n",
        "\n",
        "lstm_model.compile(optimizer='adam', loss='mse')\n",
        "lstm_model.summary()\n",
        "\n",
        "# Training LSTM\n",
        "lstm_history = lstm_model.fit(X_train, y_train, epochs=20,\n",
        "                              validation_data=(X_valid, y_valid),\n",
        "                              verbose=1)"
      ],
      "metadata": {
        "id": "8R8AawPTBhpl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Gated Recurrent Unit (GRU)\n",
        "GRU adalah versi sederhana dari LSTM dengan:\n",
        "\n",
        "- Update gate: menggabungkan forget dan input gate\n",
        "- Reset gate: menentukan seberapa banyak informasi masa lalu yang diabaikan\n",
        "- Lebih sederhana dan cepat dibandingkan LSTM"
      ],
      "metadata": {
        "id": "so0tki7BBjLz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# GRU Model\n",
        "gru_model = keras.models.Sequential([\n",
        "    keras.layers.GRU(20, return_sequences=True, input_shape=[None, 1]),\n",
        "    keras.layers.GRU(20, return_sequences=True),\n",
        "    keras.layers.GRU(20),\n",
        "    keras.layers.Dense(1)\n",
        "])\n",
        "\n",
        "gru_model.compile(optimizer='adam', loss='mse')\n",
        "\n",
        "# Training GRU\n",
        "gru_history = gru_model.fit(X_train, y_train, epochs=20,\n",
        "                            validation_data=(X_valid, y_valid),\n",
        "                            verbose=1)"
      ],
      "metadata": {
        "id": "5bBSsYdaBlt1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##3. Convolutional Neural Networks untuk Sequences\n",
        "###CNN 1D untuk Data Sekuensial\n",
        "CNN tidak hanya berguna untuk data gambar, tetapi juga dapat digunakan untuk data sekuensial dengan menggunakan konvolusi 1D.\n",
        "\n",
        "###Keuntungan CNN 1D:\n",
        "\n",
        "- Lebih cepat dibandingkan RNN\n",
        "- Dapat mendeteksi pola lokal dalam sekuens\n",
        "- Parallelizable (dapat diproses secara paralel)\n",
        "- Tidak mengalami masalah vanishing gradient"
      ],
      "metadata": {
        "id": "PRUbA0FmBnlJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# CNN 1D Model untuk time series\n",
        "cnn_model = keras.models.Sequential([\n",
        "    keras.layers.Conv1D(filters=20, kernel_size=4, strides=2, padding=\"valid\",\n",
        "                        input_shape=[None, 1]),\n",
        "    keras.layers.GRU(20, return_sequences=True),\n",
        "    keras.layers.GRU(20, return_sequences=True),\n",
        "    keras.layers.GRU(20),\n",
        "    keras.layers.Dense(1)\n",
        "])\n",
        "\n",
        "cnn_model.compile(optimizer='adam', loss='mse')\n",
        "cnn_model.summary()\n",
        "\n",
        "# Training CNN+GRU hybrid model\n",
        "cnn_history = cnn_model.fit(X_train, y_train, epochs=20,\n",
        "                            validation_data=(X_valid, y_valid),\n",
        "                            verbose=1)"
      ],
      "metadata": {
        "id": "Cp8uSvQyBtzq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###WaveNet Architecture\n",
        "WaveNet menggunakan dilated causal convolutions untuk memproses audio sequences:\n"
      ],
      "metadata": {
        "id": "_krl-bCXBvcA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Implementasi sederhana WaveNet-style model\n",
        "def wavenet_residual_block(inputs, n_filters, dilation_rate):\n",
        "    z = keras.layers.Conv1D(n_filters, 2, padding=\"causal\",\n",
        "                            dilation_rate=dilation_rate)(inputs)\n",
        "    z = keras.layers.Activation(\"tanh\")(z)\n",
        "    z = keras.layers.Conv1D(n_filters, 1)(z)\n",
        "    return keras.layers.Add()([z, inputs]), z\n",
        "\n",
        "# Build WaveNet model\n",
        "def build_wavenet_model(n_filters=32, n_blocks=3):\n",
        "    inputs = keras.layers.Input(shape=[None, 1])\n",
        "    z = keras.layers.Conv1D(n_filters, 2, padding=\"causal\")(inputs)\n",
        "\n",
        "    skip_connections = []\n",
        "    for dilation_rate in [1, 2, 4, 8] * n_blocks:\n",
        "        z, skip = wavenet_residual_block(z, n_filters, dilation_rate)\n",
        "        skip_connections.append(skip)\n",
        "\n",
        "    z = keras.layers.Activation(\"relu\")(keras.layers.Add()(skip_connections))\n",
        "    z = keras.layers.Conv1D(n_filters, 1, activation=\"relu\")(z)\n",
        "    outputs = keras.layers.Conv1D(1, 1)(z)\n",
        "\n",
        "    return keras.models.Model(inputs, outputs)\n",
        "\n",
        "wavenet_model = build_wavenet_model()\n",
        "wavenet_model.compile(optimizer='adam', loss='mse')"
      ],
      "metadata": {
        "id": "JTSfvQvCByAG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##4. Bidirectional RNNs\n",
        "###Konsep Bidirectional RNN\n",
        "Bidirectional RNN memproses sekuens dalam dua arah:\n",
        "\n",
        "- Forward: dari awal ke akhir sekuens\n",
        "- Backward: dari akhir ke awal sekuens\n",
        "\n",
        "Ini memungkinkan model untuk mengakses informasi dari masa depan dan masa lalu."
      ],
      "metadata": {
        "id": "huntydhgBzdU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Bidirectional LSTM\n",
        "bidirectional_model = keras.models.Sequential([\n",
        "    keras.layers.Bidirectional(\n",
        "        keras.layers.LSTM(20, return_sequences=True),\n",
        "        input_shape=[None, 1]\n",
        "    ),\n",
        "    keras.layers.Bidirectional(keras.layers.LSTM(20)),\n",
        "    keras.layers.Dense(1)\n",
        "])\n",
        "\n",
        "bidirectional_model.compile(optimizer='adam', loss='mse')\n",
        "bidirectional_model.summary()"
      ],
      "metadata": {
        "id": "5PmzAYzfB4aD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##5. Sequence-to-Sequence Models\n",
        "###Encoder-Decoder Architecture\n",
        "Model encoder-decoder digunakan untuk tugas di mana input dan output memiliki panjang yang berbeda:\n"
      ],
      "metadata": {
        "id": "pyPE86KlB50U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Encoder-Decoder untuk sequence prediction\n",
        "def build_seq2seq_model(n_steps_in=50, n_steps_out=10, n_features=1):\n",
        "    # Encoder\n",
        "    encoder_inputs = keras.layers.Input(shape=(n_steps_in, n_features))\n",
        "    encoder_lstm = keras.layers.LSTM(50, return_state=True)\n",
        "    encoder_outputs, state_h, state_c = encoder_lstm(encoder_inputs)\n",
        "    encoder_states = [state_h, state_c]\n",
        "\n",
        "    # Decoder\n",
        "    decoder_inputs = keras.layers.Input(shape=(n_steps_out, n_features))\n",
        "    decoder_lstm = keras.layers.LSTM(50, return_sequences=True, return_state=True)\n",
        "    decoder_outputs, _, _ = decoder_lstm(decoder_inputs, initial_state=encoder_states)\n",
        "    decoder_dense = keras.layers.Dense(n_features)\n",
        "    decoder_outputs = decoder_dense(decoder_outputs)\n",
        "\n",
        "    model = keras.models.Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
        "    return model\n",
        "\n",
        "seq2seq_model = build_seq2seq_model()\n",
        "seq2seq_model.compile(optimizer='adam', loss='mse')"
      ],
      "metadata": {
        "id": "mytiWRaDB9gh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##6. Attention Mechanism\n",
        "###Konsep Attention\n",
        "Attention mechanism memungkinkan model untuk \"memperhatikan\" bagian-bagian penting dari input sequence:\n"
      ],
      "metadata": {
        "id": "-ZTytYTwB_I_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Simple Attention Layer\n",
        "class AttentionLayer(keras.layers.Layer):\n",
        "    def __init__(self, n_units):\n",
        "        super(AttentionLayer, self).__init__()\n",
        "        self.n_units = n_units\n",
        "        self.W = keras.layers.Dense(n_units)\n",
        "        self.V = keras.layers.Dense(1)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        # inputs shape: (batch_size, time_steps, features)\n",
        "        score = self.V(tf.nn.tanh(self.W(inputs)))\n",
        "        attention_weights = tf.nn.softmax(score, axis=1)\n",
        "        context_vector = attention_weights * inputs\n",
        "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
        "        return context_vector, attention_weights\n",
        "\n",
        "# Model dengan Attention\n",
        "def build_attention_model():\n",
        "    inputs = keras.layers.Input(shape=[None, 1])\n",
        "    lstm_out = keras.layers.LSTM(50, return_sequences=True)(inputs)\n",
        "    context_vector, attention_weights = AttentionLayer(50)(lstm_out)\n",
        "    outputs = keras.layers.Dense(1)(context_vector)\n",
        "\n",
        "    model = keras.models.Model(inputs, outputs)\n",
        "    return model\n",
        "\n",
        "attention_model = build_attention_model()\n",
        "attention_model.compile(optimizer='adam', loss='mse')"
      ],
      "metadata": {
        "id": "dcosSGXsCCbg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##7. Evaluasi dan Perbandingan Model"
      ],
      "metadata": {
        "id": "GnaEMQEqCEbp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Fungsi untuk evaluasi model\n",
        "def evaluate_models(*models, model_names):\n",
        "    results = {}\n",
        "    for model, name in zip(models, model_names):\n",
        "        loss = model.evaluate(X_test, y_test, verbose=0)\n",
        "        predictions = model.predict(X_test[:100])\n",
        "        results[name] = {\n",
        "            'loss': loss,\n",
        "            'predictions': predictions\n",
        "        }\n",
        "    return results\n",
        "\n",
        "# Visualisasi hasil prediksi\n",
        "def plot_predictions(X_test, y_test, predictions, model_name):\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    for i in range(3):  # Plot 3 contoh\n",
        "        plt.subplot(1, 3, i+1)\n",
        "        plt.plot(X_test[i].flatten(), label='Input')\n",
        "        plt.axhline(y=y_test[i], color='red', linestyle='--', label='True')\n",
        "        plt.axhline(y=predictions[i], color='blue', linestyle='--', label='Predicted')\n",
        "        plt.title(f'{model_name} - Sample {i+1}')\n",
        "        plt.legend()\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "v91B-PQaCGZK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Kesimpulan\n",
        "\n",
        "Chapter 15 membahas berbagai teknik untuk memproses data sekuensial:\n",
        "\n",
        "- RNN Dasar: Cocok untuk sekuens pendek tetapi mengalami masalah vanishing gradient\n",
        "- LSTM: Mengatasi masalah vanishing gradient dengan gating mechanism yang kompleks\n",
        "- GRU: Versi sederhana LSTM dengan performa yang sebanding\n",
        "- CNN 1D: Alternatif yang lebih cepat untuk mendeteksi pola lokal dalam sekuens\n",
        "- Bidirectional RNN: Menggunakan informasi dari kedua arah untuk prediksi yang lebih baik\n",
        "- Seq2Seq: Untuk tugas dengan input dan output yang berbeda panjang\n",
        "- Attention: Memungkinkan model fokus pada bagian penting dari input\n",
        "<br><br>\n",
        "\n",
        "###Pemilihan arsitektur tergantung pada:\n",
        "\n",
        "- Panjang sekuens\n",
        "- Kompleksitas pola\n",
        "- Ketersediaan data\n",
        "- Kebutuhan kecepatan training dan inference\n",
        "<br><br>\n",
        "\n",
        "###Tips Praktis:\n",
        "\n",
        "- Gunakan LSTM/GRU untuk dependensi jangka panjang\n",
        "- CNN 1D untuk pola lokal dan kecepatan\n",
        "- Bidirectional untuk akses informasi masa depan\n",
        "- Attention untuk sekuens yang sangat panjang\n",
        "- Kombinasi CNN + RNN untuk hasil terbaik"
      ],
      "metadata": {
        "id": "t2Y7FVnVCIKv"
      }
    }
  ]
}