{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMVo0Y8CEAGf2Naq7csrTsZ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bintangnabiil/Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow/blob/main/Rangkuman_Chapter_13.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Chapter 13: Loading and Preprocessing Data with TensorFlow\n",
        "Chapter ini membahas cara efisien memuat dan memproses data menggunakan TensorFlow Data API (tf.data). Kita akan mempelajari cara menangani dataset besar, preprocessing pipeline yang efisien, dan teknik optimasi untuk meningkatkan performa training.\n",
        "\n",
        "##1. TensorFlow Data API (tf.data)\n",
        "tf.data adalah API yang dirancang khusus untuk membangun pipeline data yang efisien dan scalable. API ini memungkinkan kita untuk menangani dataset yang terlalu besar untuk dimuat sekaligus ke memory.\n",
        "\n",
        "###Keuntungan tf.data:\n",
        "\n",
        "- Streaming data dari disk untuk dataset besar\n",
        "- Parallel processing untuk preprocessing\n",
        "- Prefetching untuk mengurangi bottleneck I/O\n",
        "- Integration yang seamless dengan training loop\n",
        "- Memory efficiency untuk dataset yang sangat besar\n",
        "\n",
        "##2. Dataset Objects\n",
        "Dataset adalah abstraksi inti dari tf.data yang merepresentasikan sekuens elemen-elemen data.\n",
        "\n",
        "###Jenis-jenis Dataset:\n",
        "\n",
        "- Dataset dari tensor: Dibuat dari data yang sudah ada di memory\n",
        "- Dataset dari generator: Menggunakan Python generator\n",
        "- Dataset dari file: Langsung membaca dari file (CSV, TFRecord, dll)\n",
        "- Dataset dari directory: Untuk image datasets\n",
        "\n",
        "##3. Dataset Transformations\n",
        "Dataset dapat ditransformasi menggunakan berbagai method yang tersedia.\n",
        "\n",
        "###Transformasi umum:\n",
        "\n",
        "- .map(): Apply function ke setiap elemen\n",
        "- .filter(): Filter elemen berdasarkan kondisi\n",
        "- .batch(): Grouping elemen menjadi batch\n",
        "- .shuffle(): Mengacak urutan elemen\n",
        "- .repeat(): Mengulangi dataset\n",
        "- .take() dan .skip(): Mengambil atau melewati sejumlah elemen\n",
        "\n",
        "##4. Data Preprocessing\n",
        "Preprocessing adalah langkah penting untuk mempersiapkan data mentah menjadi format yang siap untuk training.\n",
        "\n",
        "###Teknik preprocessing umum:\n",
        "\n",
        "- Normalisasi dan scaling\n",
        "- Encoding categorical data\n",
        "- Image preprocessing (resize, crop, flip)\n",
        "Text preprocessing (tokenization, embedding)\n",
        "Feature engineering\n",
        "\n",
        "##5. Handling Different Data Formats\n",
        "TensorFlow mendukung berbagai format data dengan parser khusus.\n",
        "###Format yang didukung:\n",
        "\n",
        "- CSV files: Dengan tf.data.experimental.CsvDataset\n",
        "- TFRecord files: Format binary TensorFlow\n",
        "- Image files: JPEG, PNG dengan tf.image\n",
        "- Text files: Dengan tf.data.TextLineDataset\n",
        "\n",
        "##6. Performance Optimization\n",
        "Optimasi performa adalah kunci untuk training yang efisien, terutama untuk dataset besar.\n",
        "\n",
        "###Teknik optimasi:\n",
        "\n",
        "- Prefetching: Memuat data berikutnya saat GPU sedang training\n",
        "- Parallel processing: Menggunakan multiple cores untuk preprocessing\n",
        "- Caching: Menyimpan hasil preprocessing di memory/disk\n",
        "- Vectorization: Memproses multiple samples sekaligus\n",
        "\n",
        "##7. TFRecord Format\n",
        "TFRecord adalah format file binary yang efisien untuk menyimpan data TensorFlow.\n",
        "\n",
        "###Keuntungan TFRecord:\n",
        "\n",
        "- Efisien dalam storage dan loading\n",
        "- Mendukung compression\n",
        "- Optimal untuk sequential access\n",
        "- Built-in support untuk protocol buffers\n",
        "\n",
        "##8. Feature Engineering dengan tf.feature_column\n",
        "tf.feature_column menyediakan tools untuk feature engineering yang terintegrasi dengan model.\n",
        "\n",
        "###Jenis feature columns:\n",
        "\n",
        "- Numeric columns\n",
        "- Categorical columns\n",
        "- Embedding columns\n",
        "- Crossed columns untuk interaction features\n",
        "\n",
        "##9. Data Validation dan Quality Checks\n",
        "Memastikan kualitas data sebelum training untuk menghindari masalah during training.\n",
        "\n",
        "###Validation checks:\n",
        "\n",
        "- Missing values detection\n",
        "- Data type consistency\n",
        "- Range validation\n",
        "- Distribution analysis\n",
        "\n"
      ],
      "metadata": {
        "id": "i8NB5nC8Ugfr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Implementasi Kode\n",
        "##1. Dasar-dasar tf.data Dataset"
      ],
      "metadata": {
        "id": "q6GuUN90W7Bb"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lZxotTAAUYv9",
        "outputId": "93803c23-bb8d-49ad-9b0c-959531e93736"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset elements:\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "Dataset element spec: (TensorSpec(shape=(5,), dtype=tf.float64, name=None), TensorSpec(shape=(), dtype=tf.int64, name=None))\n",
            "Generated dataset sample:\n",
            "X shape: (5,), y: 1\n",
            "X shape: (5,), y: 1\n",
            "X shape: (5,), y: 1\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# Membuat dataset dari tensor\n",
        "data = tf.constant([1, 2, 3, 4, 5, 6, 7, 8, 9, 10])\n",
        "dataset = tf.data.Dataset.from_tensor_slices(data)\n",
        "\n",
        "print(\"Dataset elements:\")\n",
        "for element in dataset:\n",
        "    print(element.numpy())\n",
        "\n",
        "# Membuat dataset dari numpy array\n",
        "X = np.random.randn(1000, 5)\n",
        "y = np.random.randint(0, 2, 1000)\n",
        "\n",
        "dataset = tf.data.Dataset.from_tensor_slices((X, y))\n",
        "print(f\"Dataset element spec: {dataset.element_spec}\")\n",
        "\n",
        "# Membuat dataset dari generator\n",
        "def data_generator():\n",
        "    for i in range(100):\n",
        "        yield (np.random.randn(5), np.random.randint(0, 2))\n",
        "\n",
        "dataset = tf.data.Dataset.from_generator(\n",
        "    data_generator,\n",
        "    output_signature=(\n",
        "        tf.TensorSpec(shape=(5,), dtype=tf.float32),\n",
        "        tf.TensorSpec(shape=(), dtype=tf.int32)\n",
        "    )\n",
        ")\n",
        "\n",
        "print(\"Generated dataset sample:\")\n",
        "for x, y in dataset.take(3):\n",
        "    print(f\"X shape: {x.shape}, y: {y.numpy()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##2. Dataset Transformations"
      ],
      "metadata": {
        "id": "QGokJ_4QXAwd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Membuat dataset contoh\n",
        "dataset = tf.data.Dataset.range(100)\n",
        "\n",
        "# Map transformation - mengaplikasikan fungsi ke setiap elemen\n",
        "squared_dataset = dataset.map(lambda x: x ** 2)\n",
        "\n",
        "# Filter transformation - filter berdasarkan kondisi\n",
        "even_dataset = dataset.filter(lambda x: x % 2 == 0)\n",
        "\n",
        "# Batch transformation - grouping menjadi batch\n",
        "batched_dataset = dataset.batch(10)\n",
        "\n",
        "# Shuffle transformation - mengacak data\n",
        "shuffled_dataset = dataset.shuffle(buffer_size=50)\n",
        "\n",
        "# Chaining transformations\n",
        "processed_dataset = (dataset\n",
        "                    .filter(lambda x: x % 2 == 0)  # Ambil angka genap\n",
        "                    .map(lambda x: x ** 2)         # Kuadratkan\n",
        "                    .shuffle(50)                   # Acak\n",
        "                    .batch(5)                      # Batch ukuran 5\n",
        "                    .repeat(2))                    # Ulangi 2x\n",
        "\n",
        "print(\"Processed dataset batches:\")\n",
        "for batch in processed_dataset.take(3):\n",
        "    print(f\"Batch: {batch.numpy()}\")\n",
        "\n",
        "# Complex mapping dengan multiple inputs\n",
        "def preprocess_data(features, labels):\n",
        "    # Normalize features\n",
        "    features = tf.cast(features, tf.float32) / 255.0\n",
        "    # One-hot encode labels\n",
        "    labels = tf.one_hot(labels, depth=10)\n",
        "    return features, labels\n",
        "\n",
        "# Contoh dengan MNIST-like data\n",
        "X_sample = np.random.randint(0, 256, (1000, 28, 28), dtype=np.uint8)\n",
        "y_sample = np.random.randint(0, 10, 1000)\n",
        "\n",
        "dataset = tf.data.Dataset.from_tensor_slices((X_sample, y_sample))\n",
        "processed_dataset = dataset.map(preprocess_data)\n",
        "\n",
        "print(\"Preprocessed sample:\")\n",
        "for features, labels in processed_dataset.take(1):\n",
        "    print(f\"Features shape: {features.shape}, dtype: {features.dtype}\")\n",
        "    print(f\"Labels shape: {labels.shape}, sum: {tf.reduce_sum(labels).numpy()}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SL5tQsa8XCHI",
        "outputId": "5c809faf-b902-485a-9351-21619ed7b84c"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processed dataset batches:\n",
            "Batch: [ 900 1600 8100 7056 2304]\n",
            "Batch: [8836 1156 4900  484 2500]\n",
            "Batch: [9216 3600 4356 1444 6724]\n",
            "Preprocessed sample:\n",
            "Features shape: (28, 28), dtype: <dtype: 'float32'>\n",
            "Labels shape: (10,), sum: 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##3. Loading Data dari File CSV"
      ],
      "metadata": {
        "id": "R35z571VXEkQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Membuat contoh CSV file\n",
        "import tempfile\n",
        "import os\n",
        "\n",
        "# Create sample CSV data\n",
        "csv_data = \"\"\"age,income,education,purchased\n",
        "25,50000,bachelor,1\n",
        "30,75000,master,1\n",
        "22,35000,highschool,0\n",
        "28,60000,bachelor,1\n",
        "35,90000,phd,1\n",
        "24,40000,bachelor,0\n",
        "\"\"\"\n",
        "\n",
        "# Write to temporary file\n",
        "temp_dir = tempfile.mkdtemp()\n",
        "csv_file = os.path.join(temp_dir, 'sample_data.csv')\n",
        "with open(csv_file, 'w') as f:\n",
        "    f.write(csv_data)\n",
        "\n",
        "# Define column names and defaults\n",
        "column_names = ['age', 'income', 'education', 'purchased']\n",
        "column_defaults = [0, 0, '', 0]\n",
        "\n",
        "# Load CSV dataset\n",
        "def parse_csv_line(line):\n",
        "    fields = tf.io.decode_csv(line, column_defaults)\n",
        "    features = {\n",
        "        'age': fields[0],\n",
        "        'income': fields[1],\n",
        "        'education': fields[2]\n",
        "    }\n",
        "    label = fields[3]\n",
        "    return features, label\n",
        "\n",
        "# Create dataset from CSV\n",
        "dataset = tf.data.TextLineDataset(csv_file)\n",
        "dataset = dataset.skip(1)  # Skip header\n",
        "dataset = dataset.map(parse_csv_line)\n",
        "\n",
        "print(\"CSV dataset samples:\")\n",
        "for features, label in dataset:\n",
        "    print(f\"Features: {features}, Label: {label.numpy()}\")\n",
        "\n",
        "# Menggunakan tf.data.experimental.CsvDataset (alternatif)\n",
        "csv_dataset = tf.data.experimental.CsvDataset(\n",
        "    csv_file,\n",
        "    record_defaults=column_defaults,\n",
        "    header=True\n",
        ")\n",
        "\n",
        "print(\"\\nAlternative CSV loading:\")\n",
        "for record in csv_dataset.take(2):\n",
        "    print([field.numpy() for field in record])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sXS_yfSEXGSx",
        "outputId": "d1cf8041-b48c-4871-c723-fb55d523d762"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CSV dataset samples:\n",
            "Features: {'age': <tf.Tensor: shape=(), dtype=int32, numpy=25>, 'income': <tf.Tensor: shape=(), dtype=int32, numpy=50000>, 'education': <tf.Tensor: shape=(), dtype=string, numpy=b'bachelor'>}, Label: 1\n",
            "Features: {'age': <tf.Tensor: shape=(), dtype=int32, numpy=30>, 'income': <tf.Tensor: shape=(), dtype=int32, numpy=75000>, 'education': <tf.Tensor: shape=(), dtype=string, numpy=b'master'>}, Label: 1\n",
            "Features: {'age': <tf.Tensor: shape=(), dtype=int32, numpy=22>, 'income': <tf.Tensor: shape=(), dtype=int32, numpy=35000>, 'education': <tf.Tensor: shape=(), dtype=string, numpy=b'highschool'>}, Label: 0\n",
            "Features: {'age': <tf.Tensor: shape=(), dtype=int32, numpy=28>, 'income': <tf.Tensor: shape=(), dtype=int32, numpy=60000>, 'education': <tf.Tensor: shape=(), dtype=string, numpy=b'bachelor'>}, Label: 1\n",
            "Features: {'age': <tf.Tensor: shape=(), dtype=int32, numpy=35>, 'income': <tf.Tensor: shape=(), dtype=int32, numpy=90000>, 'education': <tf.Tensor: shape=(), dtype=string, numpy=b'phd'>}, Label: 1\n",
            "Features: {'age': <tf.Tensor: shape=(), dtype=int32, numpy=24>, 'income': <tf.Tensor: shape=(), dtype=int32, numpy=40000>, 'education': <tf.Tensor: shape=(), dtype=string, numpy=b'bachelor'>}, Label: 0\n",
            "\n",
            "Alternative CSV loading:\n",
            "[np.int32(25), np.int32(50000), b'bachelor', np.int32(1)]\n",
            "[np.int32(30), np.int32(75000), b'master', np.int32(1)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##4. Image Dataset Processing"
      ],
      "metadata": {
        "id": "1g5fi_jJXJ5Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Membuat contoh image dataset\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Simulasi image paths\n",
        "image_paths = [f\"image_{i}.jpg\" for i in range(10)]\n",
        "labels = np.random.randint(0, 3, 10)  # 3 classes\n",
        "\n",
        "# Create dataset dari paths dan labels\n",
        "path_dataset = tf.data.Dataset.from_tensor_slices((image_paths, labels))\n",
        "\n",
        "def load_and_preprocess_image(path, label):\n",
        "    # Dalam praktik nyata, gunakan tf.io.read_file dan tf.image.decode_image\n",
        "    # Di sini kita simulasikan dengan random image\n",
        "    image = tf.random.uniform([224, 224, 3], 0, 255, dtype=tf.float32)\n",
        "\n",
        "    # Preprocessing steps\n",
        "    image = tf.cast(image, tf.float32) / 255.0  # Normalize\n",
        "    image = tf.image.resize(image, [224, 224])   # Resize\n",
        "\n",
        "    # Data augmentation\n",
        "    image = tf.image.random_flip_left_right(image)\n",
        "    image = tf.image.random_brightness(image, 0.2)\n",
        "\n",
        "    return image, label\n",
        "\n",
        "# Apply preprocessing\n",
        "image_dataset = path_dataset.map(\n",
        "    load_and_preprocess_image,\n",
        "    num_parallel_calls=tf.data.AUTOTUNE\n",
        ")\n",
        "\n",
        "print(\"Image dataset sample:\")\n",
        "for image, label in image_dataset.take(1):\n",
        "    print(f\"Image shape: {image.shape}, Label: {label.numpy()}\")\n",
        "\n",
        "# Batch dan prefetch untuk training\n",
        "train_dataset = (image_dataset\n",
        "                .shuffle(1000)\n",
        "                .batch(4)\n",
        "                .prefetch(tf.data.AUTOTUNE))\n",
        "\n",
        "print(\"Batched image dataset:\")\n",
        "for images, labels in train_dataset.take(1):\n",
        "    print(f\"Batch images shape: {images.shape}\")\n",
        "    print(f\"Batch labels: {labels.numpy()}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8ZRGU1EUXQRw",
        "outputId": "9ccc2df0-daa4-4598-8418-3a5f853af12c"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Image dataset sample:\n",
            "Image shape: (224, 224, 3), Label: 2\n",
            "Batched image dataset:\n",
            "Batch images shape: (4, 224, 224, 3)\n",
            "Batch labels: [0 0 2 2]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##5. TFRecord Format"
      ],
      "metadata": {
        "id": "-souwOemXRDt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Membuat dan menulis TFRecord\n",
        "def create_example(features, label):\n",
        "    \"\"\"Create a tf.Example message dari features dan label.\"\"\"\n",
        "    feature = {\n",
        "        'features': tf.train.Feature(\n",
        "            float_list=tf.train.FloatList(value=features.flatten())\n",
        "        ),\n",
        "        'label': tf.train.Feature(\n",
        "            int64_list=tf.train.Int64List(value=[label])\n",
        "        ),\n",
        "        'feature_shape': tf.train.Feature(\n",
        "            int64_list=tf.train.Int64List(value=features.shape)\n",
        "        )\n",
        "    }\n",
        "    return tf.train.Example(features=tf.train.Features(feature=feature))\n",
        "\n",
        "# Generate sample data\n",
        "n_samples = 100\n",
        "X_data = np.random.randn(n_samples, 10, 5)\n",
        "y_data = np.random.randint(0, 3, n_samples)\n",
        "\n",
        "# Write TFRecord file\n",
        "tfrecord_file = os.path.join(temp_dir, 'sample.tfrecord')\n",
        "with tf.io.TFRecordWriter(tfrecord_file) as writer:\n",
        "    for i in range(n_samples):\n",
        "        example = create_example(X_data[i], y_data[i])\n",
        "        writer.write(example.SerializeToString())\n",
        "\n",
        "print(f\"TFRecord file created: {tfrecord_file}\")\n",
        "\n",
        "# Reading TFRecord\n",
        "def parse_tfrecord(example_proto):\n",
        "    \"\"\"Parse TFRecord example.\"\"\"\n",
        "    feature_description = {\n",
        "        'features': tf.io.FixedLenFeature([50], tf.float32),  # 10*5 = 50\n",
        "        'label': tf.io.FixedLenFeature([], tf.int64),\n",
        "        'feature_shape': tf.io.FixedLenFeature([2], tf.int64),\n",
        "    }\n",
        "\n",
        "    parsed_features = tf.io.parse_single_example(example_proto, feature_description)\n",
        "\n",
        "    # Reshape features back to original shape\n",
        "    features = tf.reshape(parsed_features['features'], [10, 5])\n",
        "    label = parsed_features['label']\n",
        "\n",
        "    return features, label\n",
        "\n",
        "# Load TFRecord dataset\n",
        "tfrecord_dataset = tf.data.TFRecordDataset(tfrecord_file)\n",
        "parsed_dataset = tfrecord_dataset.map(parse_tfrecord)\n",
        "\n",
        "print(\"TFRecord dataset samples:\")\n",
        "for features, label in parsed_dataset.take(2):\n",
        "    print(f\"Features shape: {features.shape}, Label: {label.numpy()}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WCi7KBFjXTsy",
        "outputId": "27c5443a-f82f-4bb7-e97e-b7a18c04e9be"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TFRecord file created: /tmp/tmpv_gt8v86/sample.tfrecord\n",
            "TFRecord dataset samples:\n",
            "Features shape: (10, 5), Label: 2\n",
            "Features shape: (10, 5), Label: 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##6. Performance Optimization"
      ],
      "metadata": {
        "id": "0IzT0IYoXVdm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Contoh dataset dengan berbagai optimasi\n",
        "def create_optimized_dataset(X, y, batch_size=32):\n",
        "    \"\"\"Create optimized dataset dengan berbagai teknik optimasi.\"\"\"\n",
        "\n",
        "    dataset = tf.data.Dataset.from_tensor_slices((X, y))\n",
        "\n",
        "    # 1. Cache - simpan hasil preprocessing\n",
        "    dataset = dataset.cache()\n",
        "\n",
        "    # 2. Shuffle dengan buffer yang cukup\n",
        "    dataset = dataset.shuffle(buffer_size=len(X))\n",
        "\n",
        "    # 3. Batch\n",
        "    dataset = dataset.batch(batch_size)\n",
        "\n",
        "    # 4. Prefetch - load batch berikutnya saat training\n",
        "    dataset = dataset.prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "    return dataset\n",
        "\n",
        "# Comparison function untuk mengukur performa\n",
        "def benchmark_dataset(dataset, num_epochs=3):\n",
        "    \"\"\"Benchmark dataset performance.\"\"\"\n",
        "    import time\n",
        "\n",
        "    start_time = time.time()\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        epoch_start = time.time()\n",
        "        batch_count = 0\n",
        "\n",
        "        for batch in dataset:\n",
        "            # Simulate processing time\n",
        "            tf.reduce_mean(batch[0])\n",
        "            batch_count += 1\n",
        "\n",
        "        epoch_time = time.time() - epoch_start\n",
        "        print(f\"Epoch {epoch + 1}: {epoch_time:.2f}s, {batch_count} batches\")\n",
        "\n",
        "    total_time = time.time() - start_time\n",
        "    print(f\"Total time: {total_time:.2f}s\")\n",
        "    return total_time\n",
        "\n",
        "# Create sample data\n",
        "X_large = np.random.randn(10000, 100)\n",
        "y_large = np.random.randint(0, 10, 10000)\n",
        "\n",
        "# Non-optimized dataset\n",
        "basic_dataset = tf.data.Dataset.from_tensor_slices((X_large, y_large)).batch(32)\n",
        "\n",
        "# Optimized dataset\n",
        "optimized_dataset = create_optimized_dataset(X_large, y_large, batch_size=32)\n",
        "\n",
        "print(\"Benchmarking basic dataset:\")\n",
        "basic_time = benchmark_dataset(basic_dataset, num_epochs=2)\n",
        "\n",
        "print(\"\\nBenchmarking optimized dataset:\")\n",
        "optimized_time = benchmark_dataset(optimized_dataset, num_epochs=2)\n",
        "\n",
        "print(f\"\\nSpeedup: {basic_time/optimized_time:.2f}x\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t8UG2iZ5XW28",
        "outputId": "4e20a83d-c3b8-454b-de93-02e5dcc3d123"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Benchmarking basic dataset:\n",
            "Epoch 1: 0.16s, 313 batches\n",
            "Epoch 2: 0.17s, 313 batches\n",
            "Total time: 0.33s\n",
            "\n",
            "Benchmarking optimized dataset:\n",
            "Epoch 1: 0.19s, 313 batches\n",
            "Epoch 2: 0.16s, 313 batches\n",
            "Total time: 0.36s\n",
            "\n",
            "Speedup: 0.93x\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##7. Advanced Data Pipeline"
      ],
      "metadata": {
        "id": "N3p76QOMXY_Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DataPipeline:\n",
        "    \"\"\"Advanced data pipeline dengan preprocessing dan augmentation.\"\"\"\n",
        "\n",
        "    def __init__(self, batch_size=32, shuffle_buffer=1000):\n",
        "        self.batch_size = batch_size\n",
        "        self.shuffle_buffer = shuffle_buffer\n",
        "\n",
        "    def preprocess_features(self, features):\n",
        "        \"\"\"Preprocess numerical features.\"\"\"\n",
        "        # Normalize features\n",
        "        features = tf.cast(features, tf.float32)\n",
        "        features = tf.nn.l2_normalize(features, axis=-1)\n",
        "\n",
        "        # Add noise untuk regularization\n",
        "        noise = tf.random.normal(tf.shape(features), stddev=0.01)\n",
        "        features = features + noise\n",
        "\n",
        "        return features\n",
        "\n",
        "    def augment_data(self, features, labels):\n",
        "        \"\"\"Data augmentation.\"\"\"\n",
        "        # Random feature dropout\n",
        "        dropout_mask = tf.random.uniform(tf.shape(features)) > 0.1\n",
        "        features = tf.where(dropout_mask, features, tf.zeros_like(features))\n",
        "\n",
        "        return features, labels\n",
        "\n",
        "    def create_pipeline(self, X, y, is_training=True):\n",
        "        \"\"\"Create complete data pipeline.\"\"\"\n",
        "        dataset = tf.data.Dataset.from_tensor_slices((X, y))\n",
        "\n",
        "        if is_training:\n",
        "            # Shuffle untuk training\n",
        "            dataset = dataset.shuffle(self.shuffle_buffer)\n",
        "\n",
        "            # Data augmentation\n",
        "            dataset = dataset.map(\n",
        "                self.augment_data,\n",
        "                num_parallel_calls=tf.data.AUTOTUNE\n",
        "            )\n",
        "\n",
        "        # Preprocessing\n",
        "        dataset = dataset.map(\n",
        "            lambda x, y: (self.preprocess_features(x), y),\n",
        "            num_parallel_calls=tf.data.AUTOTUNE\n",
        "        )\n",
        "\n",
        "        # Batch\n",
        "        dataset = dataset.batch(self.batch_size)\n",
        "\n",
        "        if is_training:\n",
        "            # Repeat untuk multiple epochs\n",
        "            dataset = dataset.repeat()\n",
        "\n",
        "        # Prefetch\n",
        "        dataset = dataset.prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "        return dataset\n",
        "\n",
        "# Menggunakan advanced pipeline\n",
        "pipeline = DataPipeline(batch_size=64, shuffle_buffer=2000)\n",
        "\n",
        "# Create sample data\n",
        "X_train = np.random.randn(5000, 20)\n",
        "y_train = np.random.randint(0, 5, 5000)\n",
        "X_val = np.random.randn(1000, 20)\n",
        "y_val = np.random.randint(0, 5, 1000)\n",
        "\n",
        "# Create datasets\n",
        "train_dataset = pipeline.create_pipeline(X_train, y_train, is_training=True)\n",
        "val_dataset = pipeline.create_pipeline(X_val, y_val, is_training=False)\n",
        "\n",
        "print(\"Advanced pipeline created successfully!\")\n",
        "\n",
        "# Test pipeline\n",
        "print(\"Training dataset sample:\")\n",
        "for features, labels in train_dataset.take(1):\n",
        "    print(f\"Features shape: {features.shape}\")\n",
        "    print(f\"Labels shape: {labels.shape}\")\n",
        "    print(f\"Feature statistics - mean: {tf.reduce_mean(features):.4f}, std: {tf.math.reduce_std(features):.4f}\")\n",
        "\n",
        "print(\"\\nValidation dataset sample:\")\n",
        "for features, labels in val_dataset.take(1):\n",
        "    print(f\"Features shape: {features.shape}\")\n",
        "    print(f\"Labels shape: {labels.shape}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q9QmW5CqXa19",
        "outputId": "af029fad-73d0-4fac-f57d-9ec05c1eeddc"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Advanced pipeline created successfully!\n",
            "Training dataset sample:\n",
            "Features shape: (64, 20)\n",
            "Labels shape: (64,)\n",
            "Feature statistics - mean: 0.0073, std: 0.2236\n",
            "\n",
            "Validation dataset sample:\n",
            "Features shape: (64, 20)\n",
            "Labels shape: (64,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##8. Handling Text Data"
      ],
      "metadata": {
        "id": "MXPT7lVgXdT9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Text preprocessing pipeline\n",
        "def create_text_pipeline(texts, labels, vocab_size=10000, max_length=100):\n",
        "    \"\"\"Create text preprocessing pipeline.\"\"\"\n",
        "\n",
        "    # Create dataset\n",
        "    dataset = tf.data.Dataset.from_tensor_slices((texts, labels))\n",
        "\n",
        "    # Text preprocessing function\n",
        "    def preprocess_text(text, label):\n",
        "        # Convert to lowercase\n",
        "        text = tf.strings.lower(text)\n",
        "\n",
        "        # Remove punctuation (simplified)\n",
        "        text = tf.strings.regex_replace(text, r'[^\\w\\s]', '')\n",
        "\n",
        "        # Split into tokens\n",
        "        tokens = tf.strings.split(text)\n",
        "\n",
        "        return tokens, label\n",
        "\n",
        "    # Apply preprocessing\n",
        "    dataset = dataset.map(preprocess_text)\n",
        "\n",
        "    # Create vocabulary\n",
        "    vocab_dataset = dataset.map(lambda tokens, label: tokens)\n",
        "    vocab_dataset = vocab_dataset.flat_map(lambda tokens: tf.data.Dataset.from_tensor_slices(tokens))\n",
        "\n",
        "    # Get unique tokens (simplified vocabulary creation)\n",
        "    # In practice, use tf.keras.utils.text_dataset_from_directory or Tokenizer\n",
        "\n",
        "    def tokenize_and_pad(tokens, label):\n",
        "        # Simple hash-based tokenization (for demo)\n",
        "        token_ids = tf.strings.to_hash_bucket_fast(tokens, vocab_size)\n",
        "\n",
        "        # Pad sequences\n",
        "        padded = tf.pad(token_ids[:max_length], [[0, max_length - tf.size(token_ids[:max_length])]])\n",
        "        padded = padded[:max_length]  # Ensure exact length\n",
        "\n",
        "        return padded, label\n",
        "\n",
        "    # Apply tokenization and padding\n",
        "    dataset = dataset.map(\n",
        "        tokenize_and_pad,\n",
        "        num_parallel_calls=tf.data.AUTOTUNE\n",
        "    )\n",
        "\n",
        "    return dataset\n",
        "\n",
        "# Sample text data\n",
        "sample_texts = [\n",
        "    \"This is a positive review about the product\",\n",
        "    \"Negative feedback about poor quality\",\n",
        "    \"Excellent service and great experience\",\n",
        "    \"Bad experience with customer support\",\n",
        "    \"Amazing product, highly recommended\"\n",
        "]\n",
        "sample_labels = [1, 0, 1, 0, 1]  # 1: positive, 0: negative\n",
        "\n",
        "# Create text pipeline\n",
        "text_dataset = create_text_pipeline(sample_texts, sample_labels)\n",
        "\n",
        "print(\"Text preprocessing pipeline:\")\n",
        "for tokens, label in text_dataset:\n",
        "    print(f\"Tokenized text shape: {tokens.shape}, Label: {label.numpy()}\")\n",
        "    print(f\"First few tokens: {tokens.numpy()[:10]}\")\n",
        "    break\n",
        "\n",
        "# Batch and prepare for training\n",
        "text_dataset = text_dataset.shuffle(100).batch(2).prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "print(\"\\nBatched text dataset:\")\n",
        "for batch_tokens, batch_labels in text_dataset.take(1):\n",
        "    print(f\"Batch tokens shape: {batch_tokens.shape}\")\n",
        "    print(f\"Batch labels: {batch_labels.numpy()}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OLDKWgZ6Xeho",
        "outputId": "19e792da-f53f-490e-f96a-f04b90c41692"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text preprocessing pipeline:\n",
            "Tokenized text shape: (100,), Label: 1\n",
            "First few tokens: [6019 8320 3939 2691 8932 2062 5354 5792    0    0]\n",
            "\n",
            "Batched text dataset:\n",
            "Batch tokens shape: (2, 100)\n",
            "Batch labels: [1 0]\n"
          ]
        }
      ]
    }
  ]
}