{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNcn3CyBhT6aI1Sex1uBRK5",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bintangnabiil/Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow/blob/main/Rangkuman_Chapter_18.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Chapter 18: Reinforcement Learning\n",
        "Reinforcement Learning (RL) adalah paradigma machine learning yang berbeda dari supervised dan unsupervised learning. Dalam RL, sebuah agent belajar untuk membuat keputusan dengan berinteraksi langsung dengan environment melalui trial dan error. Agent menerima reward atau punishment berdasarkan aksi yang diambil, dan tujuannya adalah memaksimalkan total reward yang diterima dalam jangka panjang.\n",
        "\n",
        "Konsep RL terinspirasi dari cara manusia dan hewan belajar melalui interaksi dengan lingkungan mereka. Tidak seperti supervised learning yang memerlukan labeled data, RL belajar dari konsekuensi aksi yang diambil. Ini membuat RL sangat cocok untuk masalah-masalah yang memerlukan pengambilan keputusan sekuensial dalam lingkungan yang dinamis dan tidak pasti.\n",
        "##Komponen Fundamental Reinforcement Learning\n",
        "###1. Agent dan Environment\n",
        "Agent adalah entitas yang belajar dan membuat keputusan. Agent berinteraksi dengan environment (lingkungan) yang merupakan segala sesuatu di luar agent yang dapat mempengaruhi atau dipengaruhi oleh aksi agent. Interaction loop antara agent dan environment membentuk dasar dari semua algoritma RL:\n",
        "\n",
        "- Agent mengamati state saat ini dari environment\n",
        "- Agent memilih action berdasarkan policy yang dimiliki\n",
        "- Environment merespons dengan memberikan reward dan transisi ke state baru\n",
        "- Agent menggunakan informasi ini untuk memperbaiki policy"
      ],
      "metadata": {
        "id": "pmpB5r9xo9xe"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AUHEvBtUol_T"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import gym\n",
        "from collections import defaultdict, deque\n",
        "import random\n",
        "\n",
        "# Contoh implementasi environment sederhana: Grid World\n",
        "class GridWorld:\n",
        "    def __init__(self, width=5, height=5):\n",
        "        self.width = width\n",
        "        self.height = height\n",
        "        self.agent_pos = [0, 0]  # Starting position\n",
        "        self.goal_pos = [width-1, height-1]  # Goal position\n",
        "        self.obstacles = [(2, 2), (1, 3), (3, 1)]  # Obstacle positions\n",
        "\n",
        "    def reset(self):\n",
        "        \"\"\"Reset environment ke initial state\"\"\"\n",
        "        self.agent_pos = [0, 0]\n",
        "        return self.get_state()\n",
        "\n",
        "    def get_state(self):\n",
        "        \"\"\"Return current state sebagai tuple\"\"\"\n",
        "        return tuple(self.agent_pos)\n",
        "\n",
        "    def is_valid_move(self, pos):\n",
        "        \"\"\"Check apakah posisi valid\"\"\"\n",
        "        x, y = pos\n",
        "        if x < 0 or x >= self.width or y < 0 or y >= self.height:\n",
        "            return False\n",
        "        if tuple(pos) in self.obstacles:\n",
        "            return False\n",
        "        return True\n",
        "\n",
        "    def step(self, action):\n",
        "        \"\"\"Execute action dan return (next_state, reward, done, info)\"\"\"\n",
        "        # Actions: 0=up, 1=down, 2=left, 3=right\n",
        "        moves = [(-1, 0), (1, 0), (0, -1), (0, 1)]\n",
        "\n",
        "        old_pos = self.agent_pos.copy()\n",
        "        new_pos = [self.agent_pos[0] + moves[action][0],\n",
        "                   self.agent_pos[1] + moves[action][1]]\n",
        "\n",
        "        # Check if move is valid\n",
        "        if self.is_valid_move(new_pos):\n",
        "            self.agent_pos = new_pos\n",
        "\n",
        "        # Calculate reward\n",
        "        reward = -0.1  # Small negative reward for each step (encourage efficiency)\n",
        "        done = False\n",
        "\n",
        "        if self.agent_pos == self.goal_pos:\n",
        "            reward = 10.0  # Large positive reward for reaching goal\n",
        "            done = True\n",
        "        elif tuple(self.agent_pos) in self.obstacles:\n",
        "            reward = -1.0  # Negative reward for hitting obstacle\n",
        "\n",
        "        return self.get_state(), reward, done, {}\n",
        "\n",
        "    def render(self):\n",
        "        \"\"\"Visualize current state\"\"\"\n",
        "        grid = np.zeros((self.height, self.width))\n",
        "\n",
        "        # Mark obstacles\n",
        "        for obs in self.obstacles:\n",
        "            grid[obs[1], obs[0]] = -1\n",
        "\n",
        "        # Mark goal\n",
        "        grid[self.goal_pos[1], self.goal_pos[0]] = 2\n",
        "\n",
        "        # Mark agent\n",
        "        grid[self.agent_pos[1], self.agent_pos[0]] = 1\n",
        "\n",
        "        symbols = {-1: '█', 0: '·', 1: 'A', 2: 'G'}\n",
        "        for row in grid:\n",
        "            print(''.join(symbols[int(cell)] for cell in row))\n",
        "        print()\n",
        "\n",
        "# Test environment\n",
        "env = GridWorld()\n",
        "print(\"Initial state:\")\n",
        "env.render()\n",
        "\n",
        "print(\"After moving right:\")\n",
        "state, reward, done, _ = env.step(3)\n",
        "print(f\"State: {state}, Reward: {reward}, Done: {done}\")\n",
        "env.render()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###2. State, Action, dan Reward\n",
        "State (s) merepresentasikan situasi saat ini dari environment yang dapat diamati oleh agent. State harus mengandung informasi yang cukup untuk membuat keputusan optimal. Action (a) adalah pilihan yang tersedia bagi agent pada setiap state. Reward (r) adalah signal feedback numerik yang diberikan environment kepada agent setelah mengambil action tertentu.\n",
        "\n",
        "Reward function adalah fungsi yang memetakan kombinasi state-action ke nilai reward: R(s,a) → ℝ. Design reward function sangat krusial dalam RL karena menentukan perilaku yang akan dipelajari agent. Reward engineering melibatkan:\n",
        "- Sparse rewards: Reward hanya diberikan pada goal states\n",
        "- Dense rewards: Reward diberikan pada setiap step untuk memberikan guidance\n",
        "- Shaped rewards: Reward didesain untuk memberikan informasi tambahan tentang progress"
      ],
      "metadata": {
        "id": "sq41jaJspetk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Contoh berbagai jenis reward functions\n",
        "class RewardShaping:\n",
        "    @staticmethod\n",
        "    def sparse_reward(state, goal_state):\n",
        "        \"\"\"Sparse reward: hanya reward di goal\"\"\"\n",
        "        return 100.0 if state == goal_state else 0.0\n",
        "\n",
        "    @staticmethod\n",
        "    def distance_based_reward(state, goal_state):\n",
        "        \"\"\"Dense reward berdasarkan jarak ke goal\"\"\"\n",
        "        distance = abs(state[0] - goal_state[0]) + abs(state[1] - goal_state[1])\n",
        "        return -distance  # Negative distance sebagai reward\n",
        "\n",
        "    @staticmethod\n",
        "    def potential_based_shaping(state, goal_state, gamma=0.9):\n",
        "        \"\"\"Potential-based reward shaping\"\"\"\n",
        "        # Potential function: negative distance to goal\n",
        "        potential = -(abs(state[0] - goal_state[0]) + abs(state[1] - goal_state[1]))\n",
        "        return potential\n",
        "\n",
        "    @staticmethod\n",
        "    def curiosity_reward(state, visited_states):\n",
        "        \"\"\"Curiosity-driven reward untuk exploration\"\"\"\n",
        "        visit_count = visited_states.get(state, 0)\n",
        "        return 1.0 / (1.0 + visit_count)  # Reward berkurang dengan visit count\n",
        "\n",
        "# Demonstrasi different reward functions\n",
        "goal = (4, 4)\n",
        "current_state = (2, 2)\n",
        "visited = {(2, 2): 3, (1, 1): 1}\n",
        "\n",
        "print(\"Reward Functions Comparison:\")\n",
        "print(f\"Sparse: {RewardShaping.sparse_reward(current_state, goal)}\")\n",
        "print(f\"Distance-based: {RewardShaping.distance_based_reward(current_state, goal)}\")\n",
        "print(f\"Potential-based: {RewardShaping.potential_based_shaping(current_state, goal)}\")\n",
        "print(f\"Curiosity: {RewardShaping.curiosity_reward(current_state, visited)}\")"
      ],
      "metadata": {
        "id": "omAzllkQplE-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###3. Policy dan Value Functions\n",
        "Policy (π) adalah strategi yang digunakan agent untuk memilih action pada setiap state. Policy dapat berupa:\n",
        "\n",
        "- Deterministic policy: π(s) = a (selalu memilih action yang sama untuk state tertentu)\n",
        "- Stochastic policy: π(a|s) = P(At = a | St = s) (probabilitas memilih action a pada state s)\n",
        "<br><br>\n",
        "\n",
        "Value function mengukur seberapa baik suatu state atau state-action pair:\n",
        "\n",
        "- State value function: V^π(s) = E[Gt | St = s, π] (expected return dari state s mengikuti policy π)\n",
        "- Action value function (Q-function): Q^π(s,a) = E[Gt | St = s, At = a, π] (expected return dari mengambil action a pada state s)"
      ],
      "metadata": {
        "id": "g5FHjslrpm6W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Implementasi Policy dan Value Functions\n",
        "class PolicyValueFunctions:\n",
        "    def __init__(self, num_states, num_actions):\n",
        "        self.num_states = num_states\n",
        "        self.num_actions = num_actions\n",
        "\n",
        "        # Initialize value functions\n",
        "        self.V = defaultdict(float)  # State values\n",
        "        self.Q = defaultdict(lambda: defaultdict(float))  # Action values\n",
        "\n",
        "        # Initialize policy (uniform random initially)\n",
        "        self.policy = defaultdict(lambda: np.ones(num_actions) / num_actions)\n",
        "\n",
        "    def get_action_probabilities(self, state):\n",
        "        \"\"\"Get action probabilities untuk stochastic policy\"\"\"\n",
        "        return self.policy[state]\n",
        "\n",
        "    def get_greedy_action(self, state):\n",
        "        \"\"\"Get greedy action berdasarkan Q-values\"\"\"\n",
        "        if state not in self.Q:\n",
        "            return np.random.randint(self.num_actions)\n",
        "\n",
        "        q_values = np.array([self.Q[state][a] for a in range(self.num_actions)])\n",
        "        return np.argmax(q_values)\n",
        "\n",
        "    def get_epsilon_greedy_action(self, state, epsilon=0.1):\n",
        "        \"\"\"Epsilon-greedy action selection\"\"\"\n",
        "        if np.random.random() < epsilon:\n",
        "            return np.random.randint(self.num_actions)  # Explore\n",
        "        else:\n",
        "            return self.get_greedy_action(state)  # Exploit\n",
        "\n",
        "    def update_policy_from_q(self, state, epsilon=0.1):\n",
        "        \"\"\"Update policy berdasarkan Q-values (epsilon-greedy)\"\"\"\n",
        "        probs = np.full(self.num_actions, epsilon / self.num_actions)\n",
        "        best_action = self.get_greedy_action(state)\n",
        "        probs[best_action] += 1.0 - epsilon\n",
        "        self.policy[state] = probs\n",
        "\n",
        "    def calculate_state_value(self, state):\n",
        "        \"\"\"Calculate V(s) dari Q(s,a) dan policy\"\"\"\n",
        "        if state not in self.Q:\n",
        "            return 0.0\n",
        "\n",
        "        value = 0.0\n",
        "        action_probs = self.get_action_probabilities(state)\n",
        "\n",
        "        for action in range(self.num_actions):\n",
        "            value += action_probs[action] * self.Q[state][action]\n",
        "\n",
        "        return value\n",
        "\n",
        "# Contoh penggunaan\n",
        "pv_functions = PolicyValueFunctions(num_states=25, num_actions=4)\n",
        "\n",
        "# Simulate beberapa Q-values\n",
        "test_state = (2, 2)\n",
        "pv_functions.Q[test_state][0] = 1.5  # Up\n",
        "pv_functions.Q[test_state][1] = -0.5  # Down\n",
        "pv_functions.Q[test_state][2] = 0.8   # Left\n",
        "pv_functions.Q[test_state][3] = 2.1   # Right\n",
        "\n",
        "print(\"Q-values for state (2,2):\")\n",
        "for action in range(4):\n",
        "    print(f\"Action {action}: {pv_functions.Q[test_state][action]}\")\n",
        "\n",
        "print(f\"\\nGreedy action: {pv_functions.get_greedy_action(test_state)}\")\n",
        "print(f\"Epsilon-greedy action: {pv_functions.get_epsilon_greedy_action(test_state, 0.2)}\")\n",
        "\n",
        "pv_functions.update_policy_from_q(test_state, epsilon=0.1)\n",
        "print(f\"Updated policy probabilities: {pv_functions.policy[test_state]}\")\n",
        "print(f\"State value: {pv_functions.calculate_state_value(test_state)}\")"
      ],
      "metadata": {
        "id": "c7w8WJ3jpsbh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Markov Decision Process (MDP)\n",
        "RL problems secara formal dimodelkan sebagai Markov Decision Process (MDP). MDP didefinisikan oleh tuple (S, A, P, R, γ):\n",
        "\n",
        "- S: Set of states\n",
        "- A: Set of actions\n",
        "- P: Transition probability function P(s'|s,a)\n",
        "- R: Reward function R(s,a,s')\n",
        "- γ: Discount factor (0 ≤ γ ≤ 1)\n",
        "\n",
        "Markov Property menyatakan bahwa future state hanya bergantung pada current state dan action, tidak pada history sebelumnya: P(St+1|St, At, St-1, At-1, ...) = P(St+1|St, At).\n",
        "<br><br>\n",
        "\n",
        "Discount factor γ menentukan seberapa penting future rewards dibandingkan immediate rewards. γ = 0 membuat agent hanya peduli immediate reward, sedangkan γ → 1 membuat agent mempertimbangkan long-term consequences."
      ],
      "metadata": {
        "id": "noCbcBiGpuRv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Implementasi MDP untuk Grid World\n",
        "class GridWorldMDP:\n",
        "    def __init__(self, env, gamma=0.9):\n",
        "        self.env = env\n",
        "        self.gamma = gamma\n",
        "        self.states = self._get_all_states()\n",
        "        self.actions = list(range(4))  # 0=up, 1=down, 2=left, 3=right\n",
        "\n",
        "        # Build transition model\n",
        "        self.P = self._build_transition_model()\n",
        "        self.R = self._build_reward_model()\n",
        "\n",
        "    def _get_all_states(self):\n",
        "        \"\"\"Get semua possible states\"\"\"\n",
        "        states = []\n",
        "        for x in range(self.env.width):\n",
        "            for y in range(self.env.height):\n",
        "                if (x, y) not in self.env.obstacles:\n",
        "                    states.append((x, y))\n",
        "        return states\n",
        "\n",
        "    def _build_transition_model(self):\n",
        "        \"\"\"Build transition probability model P(s'|s,a)\"\"\"\n",
        "        P = {}\n",
        "\n",
        "        for state in self.states:\n",
        "            P[state] = {}\n",
        "            for action in self.actions:\n",
        "                P[state][action] = {}\n",
        "\n",
        "                # Simulate action dari state ini\n",
        "                self.env.agent_pos = list(state)\n",
        "                next_state, _, _, _ = self.env.step(action)\n",
        "\n",
        "                # Deterministic transitions (probability = 1.0)\n",
        "                P[state][action][next_state] = 1.0\n",
        "\n",
        "        return P\n",
        "\n",
        "    def _build_reward_model(self):\n",
        "        \"\"\"Build reward model R(s,a,s')\"\"\"\n",
        "        R = {}\n",
        "\n",
        "        for state in self.states:\n",
        "            R[state] = {}\n",
        "            for action in self.actions:\n",
        "                R[state][action] = {}\n",
        "\n",
        "                # Get expected reward\n",
        "                self.env.agent_pos = list(state)\n",
        "                next_state, reward, _, _ = self.env.step(action)\n",
        "                R[state][action][next_state] = reward\n",
        "\n",
        "        return R\n",
        "\n",
        "    def get_transition_prob(self, state, action, next_state):\n",
        "        \"\"\"Get P(s'|s,a)\"\"\"\n",
        "        return self.P.get(state, {}).get(action, {}).get(next_state, 0.0)\n",
        "\n",
        "    def get_reward(self, state, action, next_state):\n",
        "        \"\"\"Get R(s,a,s')\"\"\"\n",
        "        return self.R.get(state, {}).get(action, {}).get(next_state, 0.0)\n",
        "\n",
        "    def get_expected_reward(self, state, action):\n",
        "        \"\"\"Get expected reward E[R|s,a]\"\"\"\n",
        "        expected_reward = 0.0\n",
        "\n",
        "        for next_state in self.states:\n",
        "            prob = self.get_transition_prob(state, action, next_state)\n",
        "            reward = self.get_reward(state, action, next_state)\n",
        "            expected_reward += prob * reward\n",
        "\n",
        "        return expected_reward\n",
        "\n",
        "# Contoh penggunaan MDP\n",
        "env = GridWorld(width=4, height=4)\n",
        "mdp = GridWorldMDP(env)\n",
        "\n",
        "print(\"MDP Information:\")\n",
        "print(f\"Number of states: {len(mdp.states)}\")\n",
        "print(f\"Number of actions: {len(mdp.actions)}\")\n",
        "print(f\"Discount factor: {mdp.gamma}\")\n",
        "\n",
        "# Test transition probabilities\n",
        "test_state = (1, 1)\n",
        "test_action = 3  # Right\n",
        "print(f\"\\nTransitions from state {test_state} with action {test_action}:\")\n",
        "for next_state in mdp.states:\n",
        "    prob = mdp.get_transition_prob(test_state, test_action, next_state)\n",
        "    if prob > 0:\n",
        "        reward = mdp.get_reward(test_state, test_action, next_state)\n",
        "        print(f\"  → {next_state}: P={prob:.2f}, R={reward:.2f}\")"
      ],
      "metadata": {
        "id": "q_6iFs-1p2LR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Value Iteration Algorithm\n",
        "Value Iteration adalah dynamic programming algorithm yang menghitung optimal value function dengan iteratively updating value estimates. Algorithm ini berdasarkan pada Bellman Optimality Equation:\n",
        "\n",
        "$$\n",
        "V*(s) = max_a Σ_{s'} P(s'|s,a)[R(s,a,s') + γV*(s')]\n",
        "$$\n",
        "\n",
        "Value iteration melakukan updates sampai convergence, kemudian mengekstrak optimal policy dari optimal value function."
      ],
      "metadata": {
        "id": "CYk8Ayfdp4qd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Implementasi Value Iteration\n",
        "class ValueIteration:\n",
        "    def __init__(self, mdp, theta=1e-6):\n",
        "        self.mdp = mdp\n",
        "        self.theta = theta  # Convergence threshold\n",
        "        self.V = {state: 0.0 for state in mdp.states}  # Initialize values to 0\n",
        "        self.policy = {}\n",
        "\n",
        "    def bellman_update(self, state):\n",
        "        \"\"\"Perform Bellman update untuk satu state\"\"\"\n",
        "        if state == self.mdp.env.goal_pos:\n",
        "            return 0.0  # Terminal state\n",
        "\n",
        "        max_value = float('-inf')\n",
        "\n",
        "        for action in self.mdp.actions:\n",
        "            action_value = 0.0\n",
        "\n",
        "            # Calculate E[R + γV(s')] untuk action ini\n",
        "            for next_state in self.mdp.states:\n",
        "                prob = self.mdp.get_transition_prob(state, action, next_state)\n",
        "                reward = self.mdp.get_reward(state, action, next_state)\n",
        "\n",
        "                action_value += prob * (reward + self.mdp.gamma * self.V[next_state])\n",
        "\n",
        "            max_value = max(max_value, action_value)\n",
        "\n",
        "        return max_value\n",
        "\n",
        "    def value_iteration(self, max_iterations=1000):\n",
        "        \"\"\"Run value iteration algorithm\"\"\"\n",
        "        iterations = 0\n",
        "        value_history = []\n",
        "\n",
        "        for i in range(max_iterations):\n",
        "            delta = 0.0\n",
        "            new_V = {}\n",
        "\n",
        "            # Update semua states\n",
        "            for state in self.mdp.states:\n",
        "                old_value = self.V[state]\n",
        "                new_V[state] = self.bellman_update(state)\n",
        "                delta = max(delta, abs(new_V[state] - old_value))\n",
        "\n",
        "            self.V = new_V\n",
        "            value_history.append(dict(self.V))\n",
        "            iterations += 1\n",
        "\n",
        "            # Check convergence\n",
        "            if delta < self.theta:\n",
        "                print(f\"Converged after {iterations} iterations\")\n",
        "                break\n",
        "\n",
        "        return value_history\n",
        "\n",
        "    def extract_policy(self):\n",
        "        \"\"\"Extract optimal policy dari value function\"\"\"\n",
        "        policy = {}\n",
        "\n",
        "        for state in self.mdp.states:\n",
        "            if state == tuple(self.mdp.env.goal_pos):\n",
        "                policy[state] = 0  # Arbitrary action for terminal state\n",
        "                continue\n",
        "\n",
        "            best_action = None\n",
        "            best_value = float('-inf')\n",
        "\n",
        "            for action in self.mdp.actions:\n",
        "                action_value = 0.0\n",
        "\n",
        "                for next_state in self.mdp.states:\n",
        "                    prob = self.mdp.get_transition_prob(state, action, next_state)\n",
        "                    reward = self.mdp.get_reward(state, action, next_state)\n",
        "\n",
        "                    action_value += prob * (reward + self.mdp.gamma * self.V[next_state])\n",
        "\n",
        "                if action_value > best_value:\n",
        "                    best_value = action_value\n",
        "                    best_action = action\n",
        "\n",
        "            policy[state] = best_action\n",
        "\n",
        "        self.policy = policy\n",
        "        return policy\n",
        "\n",
        "    def visualize_values(self):\n",
        "        \"\"\"Visualize value function\"\"\"\n",
        "        grid = np.zeros((self.mdp.env.height, self.mdp.env.width))\n",
        "\n",
        "        for state in self.mdp.states:\n",
        "            x, y = state\n",
        "            grid[y, x] = self.V[state]\n",
        "\n",
        "        plt.figure(figsize=(8, 6))\n",
        "        plt.imshow(grid, cmap='viridis', origin='upper')\n",
        "        plt.colorbar(label='State Value')\n",
        "        plt.title('State Values from Value Iteration')\n",
        "\n",
        "        # Add value text pada setiap cell\n",
        "        for state in self.mdp.states:\n",
        "            x, y = state\n",
        "            plt.text(x, y, f'{self.V[state]:.2f}',\n",
        "                    ha='center', va='center', color='white', fontsize=8)\n",
        "\n",
        "        plt.show()\n",
        "\n",
        "    def visualize_policy(self):\n",
        "        \"\"\"Visualize optimal policy\"\"\"\n",
        "        action_symbols = ['↑', '↓', '←', '→']\n",
        "\n",
        "        grid = np.full((self.mdp.env.height, self.mdp.env.width), ' ', dtype=str)\n",
        "\n",
        "        for state in self.mdp.states:\n",
        "            x, y = state\n",
        "            if state == tuple(self.mdp.env.goal_pos):\n",
        "                grid[y, x] = 'G'\n",
        "            elif self.policy[state] is not None:\n",
        "                grid[y, x] = action_symbols[self.policy[state]]\n",
        "\n",
        "        # Mark obstacles\n",
        "        for obs in self.mdp.env.obstacles:\n",
        "            x, y = obs\n",
        "            grid[y, x] = '█'\n",
        "\n",
        "        print(\"Optimal Policy:\")\n",
        "        for row in grid:\n",
        "            print(' '.join(row))\n",
        "\n",
        "# Run Value Iteration\n",
        "env = GridWorld(width=5, height=5)\n",
        "mdp = GridWorldMDP(env, gamma=0.9)\n",
        "vi = ValueIteration(mdp)\n",
        "\n",
        "print(\"Running Value Iteration...\")\n",
        "value_history = vi.value_iteration()\n",
        "\n",
        "print(\"\\nFinal State Values:\")\n",
        "for state in sorted(vi.V.keys()):\n",
        "    print(f\"V{state} = {vi.V[state]:.3f}\")\n",
        "\n",
        "# Extract dan visualize optimal policy\n",
        "optimal_policy = vi.extract_policy()\n",
        "print(f\"\\nOptimal Policy extracted\")\n",
        "vi.visualize_policy()\n",
        "vi.visualize_values()"
      ],
      "metadata": {
        "id": "QDqQMIQXp_i2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Policy Iteration Algorithm\n",
        "Policy Iteration adalah alternative approach yang terdiri dari dua langkah berulang:\n",
        "\n",
        "- Policy Evaluation: Menghitung value function untuk current policy\n",
        "- Policy Improvement: Update policy berdasarkan value function\n",
        "\n",
        "Policy iteration sering converge dalam fewer iterations dibandingkan value iteration, meskipun setiap iteration lebih computationally expensive."
      ],
      "metadata": {
        "id": "HZ5SH5jbqBlN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Implementasi Policy Iteration\n",
        "class PolicyIteration:\n",
        "    def __init__(self, mdp, theta=1e-6):\n",
        "        self.mdp = mdp\n",
        "        self.theta = theta\n",
        "\n",
        "        # Initialize random policy\n",
        "        self.policy = {}\n",
        "        for state in mdp.states:\n",
        "            self.policy[state] = np.random.choice(mdp.actions)\n",
        "\n",
        "        # Initialize value function\n",
        "        self.V = {state: 0.0 for state in mdp.states}\n",
        "\n",
        "    def policy_evaluation(self, max_iterations=1000):\n",
        "        \"\"\"Evaluate current policy\"\"\"\n",
        "        for i in range(max_iterations):\n",
        "            delta = 0.0\n",
        "            new_V = {}\n",
        "\n",
        "            for state in self.mdp.states:\n",
        "                if state == tuple(self.mdp.env.goal_pos):\n",
        "                    new_V[state] = 0.0  # Terminal state\n",
        "                    continue\n",
        "\n",
        "                # Calculate value untuk current policy\n",
        "                action = self.policy[state]\n",
        "                value = 0.0\n",
        "\n",
        "                for next_state in self.mdp.states:\n",
        "                    prob = self.mdp.get_transition_prob(state, action, next_state)\n",
        "                    reward = self.mdp.get_reward(state, action, next_state)\n",
        "                    value += prob * (reward + self.mdp.gamma * self.V[next_state])\n",
        "\n",
        "                new_V[state] = value\n",
        "                delta = max(delta, abs(new_V[state] - self.V[state]))\n",
        "\n",
        "            self.V = new_V\n",
        "\n",
        "            if delta < self.theta:\n",
        "                break\n",
        "\n",
        "        return i + 1  # Return number of iterations\n",
        "\n",
        "    def policy_improvement(self):\n",
        "        \"\"\"Improve policy berdasarkan current value function\"\"\"\n",
        "        policy_stable = True\n",
        "\n",
        "        for state in self.mdp.states:\n",
        "            if state == tuple(self.mdp.env.goal_pos):\n",
        "                continue\n",
        "\n",
        "            old_action = self.policy[state]\n",
        "\n",
        "            # Find best action\n",
        "            best_action = None\n",
        "            best_value = float('-inf')\n",
        "\n",
        "            for action in self.mdp.actions:\n",
        "                action_value = 0.0\n",
        "\n",
        "                for next_state in self.mdp.states:\n",
        "                    prob = self.mdp.get_transition_prob(state, action, next_state)\n",
        "                    reward = self.mdp.get_reward(state, action, next_state)\n",
        "                    action_value += prob * (reward + self.mdp.gamma * self.V[next_state])\n",
        "\n",
        "                if action_value > best_value:\n",
        "                    best_value = action_value\n",
        "                    best_action = action\n",
        "\n",
        "            self.policy[state] = best_action\n",
        "\n",
        "            if old_action != best_action:\n",
        "                policy_stable = False\n",
        "\n",
        "        return policy_stable\n",
        "\n",
        "    def policy_iteration(self, max_iterations=100):\n",
        "        \"\"\"Run policy iteration algorithm\"\"\"\n",
        "        iteration_history = []\n",
        "\n",
        "        for i in range(max_iterations):\n",
        "            print(f\"Policy Iteration {i+1}\")\n",
        "\n",
        "            # Policy Evaluation\n",
        "            eval_iterations = self.policy_evaluation()\n",
        "            print(f\"  Policy evaluation converged in {eval_iterations} iterations\")\n",
        "\n",
        "            # Policy Improvement\n",
        "            policy_stable = self.policy_improvement()\n",
        "\n",
        "            # Store current state\n",
        "            iteration_history.append({\n",
        "                'iteration': i + 1,\n",
        "                'policy': dict(self.policy),\n",
        "                'values': dict(self.V),\n",
        "                'stable': policy_stable\n",
        "            })\n",
        "\n",
        "            if policy_stable:\n",
        "                print(f\"Policy converged after {i+1} iterations\")\n",
        "                break\n",
        "\n",
        "        return iteration_history\n",
        "\n",
        "    def compare_with_value_iteration(self, vi_result):\n",
        "        \"\"\"Compare results dengan Value Iteration\"\"\"\n",
        "        print(\"\\nComparison with Value Iteration:\")\n",
        "        print(\"State Value Differences:\")\n",
        "\n",
        "        max_diff = 0.0\n",
        "        for state in self.mdp.states:\n",
        "            diff = abs(self.V[state] - vi_result.V[state])\n",
        "            max_diff = max(max_diff, diff)\n",
        "            if diff > 1e-3:  # Only show significant differences\n",
        "                print(f\"  {state}: PI={self.V[state]:.4f}, VI={vi_result.V[state]:.4f}, diff={diff:.4f}\")\n",
        "\n",
        "        print(f\"Maximum difference: {max_diff:.6f}\")\n",
        "\n",
        "        # Compare policies\n",
        "        policy_differences = 0\n",
        "        for state in self.mdp.states:\n",
        "            if self.policy[state] != vi_result.policy[state]:\n",
        "                policy_differences += 1\n",
        "                print(f\"Policy difference at {state}: PI={self.policy[state]}, VI={vi_result.policy[state]}\")\n",
        "\n",
        "        print(f\"Policy differences: {policy_differences} out of {len(self.mdp.states)} states\")\n",
        "\n",
        "# Run Policy Iteration\n",
        "print(\"Running Policy Iteration...\")\n",
        "pi = PolicyIteration(mdp, theta=1e-6)\n",
        "pi_history = pi.policy_iteration()\n",
        "\n",
        "print(\"\\nPolicy Iteration Results:\")\n",
        "action_symbols = ['↑', '↓', '←', '→']\n",
        "\n",
        "print(\"Final Policy:\")\n",
        "grid = np.full((mdp.env.height, mdp.env.width), ' ', dtype=str)\n",
        "\n",
        "for state in mdp.states:\n",
        "    x, y = state\n",
        "    if state == tuple(mdp.env.goal_pos):\n",
        "        grid[y, x] = 'G'\n",
        "    else:\n",
        "        grid[y, x] = action_symbols[pi.policy[state]]\n",
        "\n",
        "for obs in mdp.env.obstacles:\n",
        "    x, y = obs\n",
        "    grid[y, x] = '█'\n",
        "\n",
        "for row in grid:\n",
        "    print(' '.join(row))\n",
        "\n",
        "# Compare dengan Value Iteration results\n",
        "pi.compare_with_value_iteration(vi)"
      ],
      "metadata": {
        "id": "FxPdI9AgqFsz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Q-Learning Algorithm\n",
        "Q-Learning adalah model-free reinforcement learning algorithm yang dapat belajar optimal policy tanpa mengetahui transition model environment. Q-Learning menggunakan Temporal Difference (TD) learning untuk update Q-values berdasarkan experience.\n",
        "\n",
        "Q-Learning update rule:\n",
        "Q(s,a) ← Q(s,a) + α[r + γ max_a' Q(s',a') - Q(s,a)]\n",
        "\n",
        "Di mana α adalah learning rate, γ adalah discount factor, dan term dalam bracket adalah TD error."
      ],
      "metadata": {
        "id": "TfPaXXiHqItE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Implementasi Q-Learning\n",
        "class QLearning:\n",
        "    def __init__(self, num_states, num_actions, learning_rate=0.1, discount_factor=0.9,\n",
        "                 epsilon=0.1, epsilon_decay=0.995, min_epsilon=0.01):\n",
        "        self.num_actions = num_actions\n",
        "        self.lr = learning_rate\n",
        "        self.gamma = discount_factor\n",
        "        self.epsilon = epsilon\n",
        "        self.epsilon_decay = epsilon_decay\n",
        "        self.min_epsilon = min_epsilon\n",
        "\n",
        "        # Initialize Q-table\n",
        "        self.Q = defaultdict(lambda: np.zeros(num_actions))\n",
        "\n",
        "        # Tracking\n",
        "        self.episode_rewards = []\n",
        "        self.episode_lengths = []\n",
        "        self.td_errors = []\n",
        "\n",
        "    def get_action(self, state, training=True):\n",
        "        \"\"\"Get action menggunakan epsilon-greedy policy\"\"\"\n",
        "        if training and np.random.random() < self.epsilon:\n",
        "            return np.random.randint(self.num_actions)  # Explore\n",
        "        else:\n",
        "            return np.argmax(self.Q[state])  # Exploit\n",
        "\n",
        "    def update(self, state, action, reward, next_state, done):\n",
        "        \"\"\"Update Q-value menggunakan Q-learning rule\"\"\"\n",
        "        current_q = self.Q[state][action]\n",
        "\n",
        "        if done:\n",
        "            target_q = reward  # No future rewards\n",
        "        else:\n",
        "            target_q = reward + self.gamma * np.max(self.Q[next_state])\n",
        "\n",
        "        # Calculate TD error\n",
        "        td_error = target_q - current_q\n",
        "        self.td_errors.append(abs(td_error))\n",
        "\n",
        "        # Update Q-value\n",
        "        self.Q[state][action] += self.lr * td_error\n",
        "\n",
        "        return td_error\n",
        "\n",
        "    def decay_epsilon(self):\n",
        "        \"\"\"Decay exploration rate\"\"\"\n",
        "        self.epsilon = max(self.min_epsilon, self.epsilon * self.epsilon_decay)\n",
        "\n",
        "    def train(self, env, num_episodes=1000, max_steps_per_episode=200):\n",
        "        \"\"\"Train Q-learning agent\"\"\"\n",
        "        print(f\"Training Q-Learning for {num_episodes} episodes...\")\n",
        "\n",
        "        for episode in range(num_episodes):\n",
        "            state = env.reset()\n",
        "            total_reward = 0\n",
        "            steps = 0\n",
        "\n",
        "            for step in range(max_steps_per_episode):\n",
        "                # Get action\n",
        "                action = self.get_action(state, training=True)\n",
        "\n",
        "                # Take action\n",
        "                next_state, reward, done, _ = env.step(action)\n",
        "\n",
        "                # Update Q-value\n",
        "                td_error = self.update(state, action, reward, next_state, done)\n",
        "\n",
        "                # Update state\n",
        "                state = next_state\n",
        "                total_reward += reward\n",
        "                steps += 1\n",
        "\n",
        "                if done:\n",
        "                    break\n",
        "\n",
        "            # Store episode statistics\n",
        "            self.episode_rewards.append(total_reward)\n",
        "            self.episode_lengths.append(steps)\n",
        "\n",
        "            # Decay epsilon\n",
        "            self.decay_epsilon()\n",
        "\n",
        "            # Print progress\n",
        "            if (episode + 1) % 100 == 0:\n",
        "                avg_reward = np.mean(self.episode_rewards[-100:])\n",
        "                avg_length = np.mean(self.episode_lengths[-100:])\n",
        "                print(f\"Episode {episode+1}: Avg Reward = {avg_reward:.2f}, \"\n",
        "                      f\"Avg Length = {avg_length:.1f}, Epsilon = {self.epsilon:.3f}\")\n",
        "\n",
        "        print(\"Training completed!\")\n",
        "\n",
        "    def test(self, env, num_episodes=10, render=False):\n",
        "        \"\"\"Test trained agent\"\"\"\n",
        "        test_rewards = []\n",
        "        test_lengths = []\n",
        "\n",
        "        for episode in range(num_episodes):\n",
        "            state = env.reset()\n",
        "            total_reward = 0\n",
        "            steps = 0\n",
        "\n",
        "            if render:\n",
        "                print(f\"\\nTest Episode {episode + 1}:\")\n",
        "                env.render()\n",
        "\n",
        "            while True:\n",
        "                action = self.get_action(state, training=False)  # No exploration\n",
        "                next_state, reward, done, _ = env.step(action)\n",
        "\n",
        "                state = next_state\n",
        "                total_reward += reward\n",
        "                steps += 1\n",
        "\n",
        "                if render:\n",
        "                    print(f\"Action: {['Up', 'Down', 'Left', 'Right'][action]}\")\n",
        "                    env.render()\n",
        "\n",
        "                if done or steps > 200:\n",
        "                    break\n",
        "\n",
        "            test_rewards.append(total_reward)\n",
        "            test_lengths.append(steps)\n",
        "\n",
        "            if render:\n",
        "                print(f\"Episode {episode + 1}: Reward = {total_reward}, Steps = {steps}\")\n",
        "\n",
        "        avg_reward = np.mean(test_rewards)\n",
        "        avg_length = np.mean(test_lengths)\n",
        "\n",
        "        print(f\"\\nTest Results over {num_episodes} episodes:\")\n",
        "        print(f\"Average Reward: {avg_reward:.2f} ± {np.std(test_rewards):.2f}\")\n",
        "        print(f\"Average Length: {avg_length:.1f} ± {np.std(test_lengths):.1f}\")\n",
        "\n",
        "        return test_rewards, test_lengths\n",
        "\n",
        "    def plot_training_progress(self):\n",
        "        \"\"\"Plot training metrics\"\"\"\n",
        "        fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
        "\n",
        "        # Episode rewards\n",
        "        axes[0, 0].plot(self.episode_rewards, alpha=0.3, label='Episode Reward')\n",
        "\n",
        "        # Moving average\n",
        "        window = min(100, len(self.episode_rewards) // 10)\n",
        "        if window > 1:\n",
        "            moving_avg = np.convolve(self.episode_rewards, np.ones(window)/window, mode='valid')\n",
        "            axes[0, 0].plot(range(window-1, len(self.episode_rewards)), moving_avg,\n",
        "                           label=f'Moving Average ({window})', linewidth=2)\n",
        "\n",
        "        axes[0, 0].set_xlabel('Episode')\n",
        "        axes[0, 0].set_ylabel('Reward')\n",
        "        axes[0, 0].set_title('Episode Rewards')\n",
        "        axes[0, 0].legend()\n",
        "        axes[0, 0].grid(True, alpha=0.3)\n",
        "\n",
        "        # Episode lengths\n",
        "        axes[0, 1].plot(self.episode_lengths, alpha=0.3, label='Episode Length')\n",
        "\n",
        "        if window > 1:\n",
        "            moving_avg_length = np.convolve(self.episode_lengths, np.ones(window)/window, mode='valid')\n",
        "            axes[0, 1].plot(range(window-1, len(self.episode_lengths)), moving_avg_length,\n",
        "                           label=f'Moving Average ({window})', linewidth=2)\n",
        "\n",
        "        axes[0, 1].set_xlabel('Episode')\n",
        "        axes[0, 1].set_ylabel('Steps')\n",
        "        axes[0, 1].set_title('Episode Lengths')\n",
        "        axes[0, 1].legend()\n",
        "        axes[0, 1].grid(True, alpha=0.3)\n",
        "\n",
        "        # TD Errors\n",
        "        if self.td_errors:\n",
        "            axes[1, 0].plot(self.td_errors, alpha=0.3)\n",
        "\n",
        "            if len(self.td_errors) > window:\n",
        "                moving_avg_td = np.convolve(self.td_errors, np.ones(window)/window, mode='valid')\n",
        "                axes[1, 0].plot(range(window-1, len(self.td_errors)), moving_avg_td,\n",
        "                               linewidth=2, color='red')\n",
        "\n",
        "            axes[1, 0].set_xlabel('Update Step')\n",
        "            axes[1, 0].set_ylabel('|TD Error|')\n",
        "            axes[1, 0].set_title('Temporal Difference Errors')\n",
        "            axes[1, 0].grid(True, alpha=0.3)\n",
        "\n",
        "        # Q-value heatmap untuk sample state\n",
        "        sample_states = list(self.Q.keys())[:min(20, len(self.Q))]\n",
        "        if sample_states:\n",
        "            q_matrix = np.array([self.Q[state] for state in sample_states])\n",
        "            im = axes[1, 1].imshow(q_matrix, cmap='viridis', aspect='auto')\n",
        "            axes[1, 1].set_xlabel('Action')\n",
        "            axes[1, 1].set_ylabel('State (sample)')\n",
        "            axes[1, 1].set_title('Q-Values Heatmap (Sample States)')\n",
        "            plt.colorbar(im, ax=axes[1, 1])\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "    def get_policy(self):\n",
        "        \"\"\"Extract greedy policy dari Q-values\"\"\"\n",
        "        policy = {}\n",
        "        for state in self.Q:\n",
        "            policy[state] = np.argmax(self.Q[state])\n",
        "        return policy\n",
        "\n",
        "    def visualize_q_values(self, env):\n",
        "        \"\"\"Visualize Q-values untuk grid world\"\"\"\n",
        "        action_symbols = ['↑', '↓', '←', '→']\n",
        "\n",
        "        print(\"Q-Values and Policy:\")\n",
        "        print(\"Format: [Up, Down, Left, Right] -> Best Action\")\n",
        "\n",
        "        for y in range(env.height):\n",
        "            for x in range(env.width):\n",
        "                state = (x, y)\n",
        "\n",
        "                if state in env.obstacles:\n",
        "                    print(\"   OBSTACLE  \", end=\"  \")\n",
        "                elif state == tuple(env.goal_pos):\n",
        "                    print(\"    GOAL     \", end=\"  \")\n",
        "                elif state in self.Q:\n",
        "                    q_vals = self.Q[state]\n",
        "                    best_action = np.argmax(q_vals)\n",
        "                    print(f\"[{q_vals[0]:4.1f},{q_vals[1]:4.1f},{q_vals[2]:4.1f},{q_vals[3]:4.1f}]{action_symbols[best_action]}\", end=\" \")\n",
        "                else:\n",
        "                    print(\"    UNVIS    \", end=\"  \")\n",
        "            print()\n",
        "\n",
        "# Implementasi dan training Q-Learning\n",
        "print(\"=\"*60)\n",
        "print(\"Q-LEARNING DEMONSTRATION\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Create environment dan agent\n",
        "env = GridWorld(width=5, height=5)\n",
        "q_agent = QLearning(num_states=25, num_actions=4,\n",
        "                   learning_rate=0.1, discount_factor=0.9,\n",
        "                   epsilon=0.9, epsilon_decay=0.995)\n",
        "\n",
        "# Train agent\n",
        "q_agent.train(env, num_episodes=500, max_steps_per_episode=100)\n",
        "\n",
        "# Test trained agent\n",
        "print(\"\\nTesting trained agent:\")\n",
        "test_rewards, test_lengths = q_agent.test(env, num_episodes=5, render=True)\n",
        "\n",
        "# Visualize results\n",
        "q_agent.plot_training_progress()\n",
        "q_agent.visualize_q_values(env)\n",
        "\n",
        "# Compare Q-Learning policy dengan optimal policy\n",
        "print(\"\\nComparison with Value Iteration:\")\n",
        "q_policy = q_agent.get_policy()\n",
        "differences = 0\n",
        "\n",
        "for state in vi.policy:\n",
        "    if state in q_policy:\n",
        "        if q_policy[state] != vi.policy[state]:\n",
        "            differences += 1\n",
        "            print(f\"Policy difference at {state}: Q-Learning={q_policy[state]}, VI={vi.policy[state]}\")\n",
        "\n",
        "print(f\"Total policy differences: {differences}\")"
      ],
      "metadata": {
        "id": "qTA9jAL8qNFT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##SARSA Algorithm\n",
        "SARSA (State-Action-Reward-State-Action) adalah on-policy TD learning algorithm yang update Q-values berdasarkan action yang benar-benar akan diambil oleh current policy, bukan action terbaik seperti pada Q-Learning.\n",
        "\n",
        "SARSA update rule:\n",
        "Q(s,a) ← Q(s,a) + α[r + γQ(s',a') - Q(s,a)]\n",
        "\n",
        "Perbedaan utama dengan Q-Learning adalah SARSA menggunakan Q(s',a') dimana a' adalah action yang akan diambil, sedangkan Q-Learning menggunakan max_a' Q(s',a')."
      ],
      "metadata": {
        "id": "5a2RamWgs4Oo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Implementasi SARSA Algorithm\n",
        "class SARSA:\n",
        "    def __init__(self, num_states, num_actions, learning_rate=0.1, discount_factor=0.9,\n",
        "                 epsilon=0.1, epsilon_decay=0.995, min_epsilon=0.01):\n",
        "        self.num_actions = num_actions\n",
        "        self.lr = learning_rate\n",
        "        self.gamma = discount_factor\n",
        "        self.epsilon = epsilon\n",
        "        self.epsilon_decay = epsilon_decay\n",
        "        self.min_epsilon = min_epsilon\n",
        "\n",
        "        # Initialize Q-table\n",
        "        self.Q = defaultdict(lambda: np.zeros(num_actions))\n",
        "\n",
        "        # Tracking\n",
        "        self.episode_rewards = []\n",
        "        self.episode_lengths = []\n",
        "        self.td_errors = []\n",
        "\n",
        "    def get_action(self, state, training=True):\n",
        "        \"\"\"Epsilon-greedy action selection\"\"\"\n",
        "        if training and np.random.random() < self.epsilon:\n",
        "            return np.random.randint(self.num_actions)\n",
        "        else:\n",
        "            return np.argmax(self.Q[state])\n",
        "\n",
        "    def update(self, state, action, reward, next_state, next_action, done):\n",
        "        \"\"\"Update Q-value menggunakan SARSA rule\"\"\"\n",
        "        current_q = self.Q[state][action]\n",
        "\n",
        "        if done:\n",
        "            target_q = reward\n",
        "        else:\n",
        "            target_q = reward + self.gamma * self.Q[next_state][next_action]\n",
        "\n",
        "        td_error = target_q - current_q\n",
        "        self.td_errors.append(abs(td_error))\n",
        "\n",
        "        self.Q[state][action] += self.lr * td_error\n",
        "\n",
        "        return td_error\n",
        "\n",
        "    def decay_epsilon(self):\n",
        "        \"\"\"Decay exploration rate\"\"\"\n",
        "        self.epsilon = max(self.min_epsilon, self.epsilon * self.epsilon_decay)\n",
        "\n",
        "    def train(self, env, num_episodes=1000, max_steps_per_episode=200):\n",
        "        \"\"\"Train SARSA agent\"\"\"\n",
        "        print(f\"Training SARSA for {num_episodes} episodes...\")\n",
        "\n",
        "        for episode in range(num_episodes):\n",
        "            state = env.reset()\n",
        "            action = self.get_action(state, training=True)  # Initial action\n",
        "            total_reward = 0\n",
        "            steps = 0\n",
        "\n",
        "            for step in range(max_steps_per_episode):\n",
        "                # Take action\n",
        "                next_state, reward, done, _ = env.step(action)\n",
        "\n",
        "                if done:\n",
        "                    # Update with terminal state\n",
        "                    td_error = self.update(state, action, reward, next_state, None, done)\n",
        "                    total_reward += reward\n",
        "                    steps += 1\n",
        "                    break\n",
        "\n",
        "                # Get next action (important: this is what makes it SARSA)\n",
        "                next_action = self.get_action(next_state, training=True)\n",
        "\n",
        "                # Update Q-value\n",
        "                td_error = self.update(state, action, reward, next_state, next_action, done)\n",
        "\n",
        "                # Move to next state-action pair\n",
        "                state = next_state\n",
        "                action = next_action\n",
        "                total_reward += reward\n",
        "                steps += 1\n",
        "\n",
        "            self.episode_rewards.append(total_reward)\n",
        "            self.episode_lengths.append(steps)\n",
        "            self.decay_epsilon()\n",
        "\n",
        "            if (episode + 1) % 100 == 0:\n",
        "                avg_reward = np.mean(self.episode_rewards[-100:])\n",
        "                avg_length = np.mean(self.episode_lengths[-100:])\n",
        "                print(f\"Episode {episode+1}: Avg Reward = {avg_reward:.2f}, \"\n",
        "                      f\"Avg Length = {avg_length:.1f}, Epsilon = {self.epsilon:.3f}\")\n",
        "\n",
        "        print(\"SARSA Training completed!\")\n",
        "\n",
        "    def compare_with_qlearning(self, q_learning_agent, env, num_test_episodes=100):\n",
        "        \"\"\"Compare SARSA performance dengan Q-Learning\"\"\"\n",
        "        print(\"\\nComparing SARSA vs Q-Learning:\")\n",
        "\n",
        "        # Test SARSA\n",
        "        sarsa_rewards = []\n",
        "        for episode in range(num_test_episodes):\n",
        "            state = env.reset()\n",
        "            total_reward = 0\n",
        "            steps = 0\n",
        "\n",
        "            while steps < 200:\n",
        "                action = self.get_action(state, training=False)\n",
        "                next_state, reward, done, _ = env.step(action)\n",
        "                state = next_state\n",
        "                total_reward += reward\n",
        "                steps += 1\n",
        "\n",
        "                if done:\n",
        "                    break\n",
        "\n",
        "            sarsa_rewards.append(total_reward)\n",
        "\n",
        "        # Test Q-Learning\n",
        "        qlearning_rewards = []\n",
        "        for episode in range(num_test_episodes):\n",
        "            state = env.reset()\n",
        "            total_reward = 0\n",
        "            steps = 0\n",
        "\n",
        "            while steps < 200:\n",
        "                action = q_learning_agent.get_action(state, training=False)\n",
        "                next_state, reward, done, _ = env.step(action)\n",
        "                state = next_state\n",
        "                total_reward += reward\n",
        "                steps += 1\n",
        "\n",
        "                if done:\n",
        "                    break\n",
        "\n",
        "            qlearning_rewards.append(total_reward)\n",
        "\n",
        "        # Statistical comparison\n",
        "        sarsa_mean = np.mean(sarsa_rewards)\n",
        "        sarsa_std = np.std(sarsa_rewards)\n",
        "        qlearning_mean = np.mean(qlearning_rewards)\n",
        "        qlearning_std = np.std(qlearning_rewards)\n",
        "\n",
        "        print(f\"SARSA: {sarsa_mean:.2f} ± {sarsa_std:.2f}\")\n",
        "        print(f\"Q-Learning: {qlearning_mean:.2f} ± {qlearning_std:.2f}\")\n",
        "\n",
        "        # Plot comparison\n",
        "        plt.figure(figsize=(12, 5))\n",
        "\n",
        "        plt.subplot(1, 2, 1)\n",
        "        plt.hist(sarsa_rewards, alpha=0.7, label='SARSA', bins=20)\n",
        "        plt.hist(qlearning_rewards, alpha=0.7, label='Q-Learning', bins=20)\n",
        "        plt.xlabel('Episode Reward')\n",
        "        plt.ylabel('Frequency')\n",
        "        plt.title('Reward Distribution Comparison')\n",
        "        plt.legend()\n",
        "\n",
        "        plt.subplot(1, 2, 2)\n",
        "        episodes = range(len(self.episode_rewards))\n",
        "        plt.plot(episodes, self.episode_rewards, alpha=0.3, label='SARSA')\n",
        "        plt.plot(range(len(q_learning_agent.episode_rewards)),\n",
        "                q_learning_agent.episode_rewards, alpha=0.3, label='Q-Learning')\n",
        "\n",
        "        # Moving averages\n",
        "        window = 50\n",
        "        if len(self.episode_rewards) > window:\n",
        "            sarsa_ma = np.convolve(self.episode_rewards, np.ones(window)/window, mode='valid')\n",
        "            plt.plot(range(window-1, len(self.episode_rewards)), sarsa_ma,\n",
        "                    label='SARSA (MA)', linewidth=2)\n",
        "\n",
        "        if len(q_learning_agent.episode_rewards) > window:\n",
        "            q_ma = np.convolve(q_learning_agent.episode_rewards, np.ones(window)/window, mode='valid')\n",
        "            plt.plot(range(window-1, len(q_learning_agent.episode_rewards)), q_ma,\n",
        "                    label='Q-Learning (MA)', linewidth=2)\n",
        "\n",
        "        plt.xlabel('Episode')\n",
        "        plt.ylabel('Reward')\n",
        "        plt.title('Training Progress Comparison')\n",
        "        plt.legend()\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "# Train SARSA agent\n",
        "sarsa_agent = SARSA(num_states=25, num_actions=4,\n",
        "                   learning_rate=0.1, discount_factor=0.9,\n",
        "                   epsilon=0.9, epsilon_decay=0.995)\n",
        "\n",
        "sarsa_agent.train(env, num_episodes=500, max_steps_per_episode=100)\n",
        "\n",
        "# Compare SARSA dengan Q-Learning\n",
        "sarsa_agent.compare_with_qlearning(q_agent, env, num_test_episodes=100)"
      ],
      "metadata": {
        "id": "a1SyYRCks7da"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Deep Q-Network (DQN)\n",
        "Deep Q-Network (DQN) menggunakan neural network untuk aproximasi Q-function, memungkinkan RL untuk menangani high-dimensional state spaces. DQN memperkenalkan beberapa teknik penting:\n",
        "\n",
        "- Experience Replay: Menyimpan experiences dalam replay buffer dan sampling secara random untuk training\n",
        "- Target Network: Menggunakan separate network untuk target Q-values yang di-update secara periodik\n",
        "- Double DQN: Mengurangi overestimation bias dengan memisahkan action selection dan evaluation"
      ],
      "metadata": {
        "id": "FQoCsxWWs-Ul"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from collections import deque\n",
        "\n",
        "# Implementasi Deep Q-Network\n",
        "class DQN:\n",
        "    def __init__(self, state_size, action_size, learning_rate=0.001,\n",
        "                 gamma=0.99, epsilon=1.0, epsilon_decay=0.995, min_epsilon=0.01,\n",
        "                 memory_size=10000, batch_size=32, target_update_freq=100):\n",
        "\n",
        "        self.state_size = state_size\n",
        "        self.action_size = action_size\n",
        "        self.lr = learning_rate\n",
        "        self.gamma = gamma\n",
        "        self.epsilon = epsilon\n",
        "        self.epsilon_decay = epsilon_decay\n",
        "        self.min_epsilon = min_epsilon\n",
        "        self.batch_size = batch_size\n",
        "        self.target_update_freq = target_update_freq\n",
        "\n",
        "        # Experience replay memory\n",
        "        self.memory = deque(maxlen=memory_size)\n",
        "\n",
        "        # Neural networks\n",
        "        self.q_network = self._build_network()\n",
        "        self.target_network = self._build_network()\n",
        "\n",
        "        # Initialize target network\n",
        "        self.update_target_network()\n",
        "\n",
        "        # Training statistics\n",
        "        self.losses = []\n",
        "        self.episode_rewards = []\n",
        "        self.training_step = 0\n",
        "\n",
        "    def _build_network(self):\n",
        "        \"\"\"Build neural network untuk Q-function approximation\"\"\"\n",
        "        model = tf.keras.Sequential([\n",
        "            tf.keras.layers.Dense(128, activation='relu', input_shape=(self.state_size,)),\n",
        "            tf.keras.layers.Dense(128, activation='relu'),\n",
        "            tf.keras.layers.Dense(64, activation='relu'),\n",
        "            tf.keras.layers.Dense(self.action_size, activation='linear')\n",
        "        ])\n",
        "\n",
        "        model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=self.lr),\n",
        "                     loss='mse')\n",
        "\n",
        "        return model\n",
        "\n",
        "    def remember(self, state, action, reward, next_state, done):\n",
        "        \"\"\"Store experience dalam replay memory\"\"\"\n",
        "        self.memory.append((state, action, reward, next_state, done))\n",
        "\n",
        "    def get_action(self, state, training=True):\n",
        "        \"\"\"Epsilon-greedy action selection\"\"\"\n",
        "        if training and np.random.random() < self.epsilon:\n",
        "            return np.random.randint(self.action_size)\n",
        "\n",
        "        q_values = self.q_network.predict(state.reshape(1, -1), verbose=0)\n",
        "        return np.argmax(q_values[0])\n",
        "\n",
        "    def replay(self):\n",
        "        \"\"\"Train network dengan batch dari experience replay\"\"\"\n",
        "        if len(self.memory) < self.batch_size:\n",
        "            return\n",
        "\n",
        "        # Sample random batch\n",
        "        batch = random.sample(list(self.memory), self.batch_size)\n",
        "        states = np.array([e[0] for e in batch])\n",
        "        actions = np.array([e[1] for e in batch])\n",
        "        rewards = np.array([e[2] for e in batch])\n",
        "        next_states = np.array([e[3] for e in batch])\n",
        "        dones = np.array([e[4] for e in batch])\n",
        "\n",
        "        # Current Q-values\n",
        "        current_q_values = self.q_network.predict(states, verbose=0)\n",
        "\n",
        "        # Target Q-values dari target network\n",
        "        target_q_values = self.target_network.predict(next_states, verbose=0)\n",
        "\n",
        "        # Update Q-values untuk actions yang diambil\n",
        "        for i in range(self.batch_size):\n",
        "            if dones[i]:\n",
        "                current_q_values[i][actions[i]] = rewards[i]\n",
        "            else:\n",
        "                current_q_values[i][actions[i]] = rewards[i] + self.gamma * np.max(target_q_values[i])\n",
        "\n",
        "        # Train network\n",
        "        history = self.q_network.fit(states, current_q_values,\n",
        "                                   epochs=1, verbose=0, batch_size=self.batch_size)\n",
        "\n",
        "        self.losses.append(history.history['loss'][0])\n",
        "        self.training_step += 1\n",
        "\n",
        "        # Update target network\n",
        "        if self.training_step % self.target_update_freq == 0:\n",
        "            self.update_target_network()\n",
        "\n",
        "        # Decay epsilon\n",
        "        if self.epsilon > self.min_epsilon:\n",
        "            self.epsilon *= self.epsilon_decay\n",
        "\n",
        "    def update_target_network(self):\n",
        "        \"\"\"Copy weights dari main network ke target network\"\"\"\n",
        "        self.target_network.set_weights(self.q_network.get_weights())\n",
        "\n",
        "    def state_to_vector(self, state, env):\n",
        "        \"\"\"Convert grid position ke feature vector\"\"\"\n",
        "        # One-hot encoding untuk position\n",
        "        vector = np.zeros(env.width * env.height)\n",
        "        x, y = state\n",
        "        index = y * env.width + x\n",
        "        vector[index] = 1.0\n",
        "\n",
        "        # Add distance features\n",
        "        goal_x, goal_y = env.goal_pos\n",
        "        distance_features = [\n",
        "            abs(x - goal_x) / env.width,   # Normalized x distance\n",
        "            abs(y - goal_y) / env.height,  # Normalized y distance\n",
        "            (abs(x - goal_x) + abs(y - goal_y)) / (env.width + env.height)  # Manhattan distance\n",
        "        ]\n",
        "\n",
        "        return np.concatenate([vector, distance_features])\n",
        "\n",
        "    def train(self, env, num_episodes=1000, max_steps_per_episode=200):\n",
        "        \"\"\"Train DQN agent\"\"\"\n",
        "        print(f\"Training DQN for {num_episodes} episodes...\")\n",
        "\n",
        "        state_size = env.width * env.height + 3  # Position + distance features\n",
        "\n",
        "        for episode in range(num_episodes):\n",
        "            state = env.reset()\n",
        "            state_vector = self.state_to_vector(state, env)\n",
        "            total_reward = 0\n",
        "            steps = 0\n",
        "\n",
        "            for step in range(max_steps_per_episode):\n",
        "                # Get action\n",
        "                action = self.get_action(state_vector, training=True)\n",
        "\n",
        "                # Take action\n",
        "                next_state, reward, done, _ = env.step(action)\n",
        "                next_state_vector = self.state_to_vector(next_state, env)\n",
        "\n",
        "                # Store experience\n",
        "                self.remember(state_vector, action, reward, next_state_vector, done)\n",
        "\n",
        "                # Train network\n",
        "                self.replay()\n",
        "\n",
        "                state_vector = next_state_vector\n",
        "                total_reward += reward\n",
        "                steps += 1\n",
        "\n",
        "                if done:\n",
        "                    break\n",
        "\n",
        "            self.episode_rewards.append(total_reward)\n",
        "\n",
        "            # Print progress\n",
        "            if (episode + 1) % 100 == 0:\n",
        "                avg_reward = np.mean(self.episode_rewards[-100:])\n",
        "                avg_loss = np.mean(self.losses[-100:]) if self.losses else 0\n",
        "                print(f\"Episode {episode+1}: Avg Reward = {avg_reward:.2f}, \"\n",
        "                      f\"Avg Loss = {avg_loss:.4f}, Epsilon = {self.epsilon:.3f}\")\n",
        "\n",
        "        print(\"DQN Training completed!\")\n",
        "\n",
        "    def test(self, env, num_episodes=10, render=False):\n",
        "        \"\"\"Test trained DQN agent\"\"\"\n",
        "        test_rewards = []\n",
        "\n",
        "        for episode in range(num_episodes):\n",
        "            state = env.reset()\n",
        "            state_vector = self.state_to_vector(state, env)\n",
        "            total_reward = 0\n",
        "            steps = 0\n",
        "\n",
        "            if render:\n",
        "                print(f\"\\nDQN Test Episode {episode + 1}:\")\n",
        "                env.render()\n",
        "\n",
        "            while steps < 200:\n",
        "                action = self.get_action(state_vector, training=False)\n",
        "                next_state, reward, done, _ = env.step(action)\n",
        "                next_state_vector = self.state_to_vector(next_state, env)\n",
        "\n",
        "                state_vector = next_state_vector\n",
        "                total_reward += reward\n",
        "                steps += 1\n",
        "\n",
        "                if render:\n",
        "                    print(f\"Action: {['Up', 'Down', 'Left', 'Right'][action]}\")\n",
        "                    env.render()\n",
        "\n",
        "                if done:\n",
        "                    break\n",
        "\n",
        "            test_rewards.append(total_reward)\n",
        "\n",
        "            if render:\n",
        "                print(f\"Episode {episode + 1}: Reward = {total_reward}, Steps = {steps}\")\n",
        "\n",
        "        avg_reward = np.mean(test_rewards)\n",
        "        print(f\"\\nDQN Test Results: Average Reward = {avg_reward:.2f} ± {np.std(test_rewards):.2f}\")\n",
        "\n",
        "        return test_rewards\n",
        "\n",
        "    def plot_training_progress(self):\n",
        "        \"\"\"Plot DQN training metrics\"\"\"\n",
        "        fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
        "\n",
        "        # Episode rewards\n",
        "        axes[0, 0].plot(self.episode_rewards, alpha=0.3, label='Episode Reward')\n",
        "\n",
        "        window = min(100, len(self.episode_rewards) // 10)\n",
        "        if window > 1:\n",
        "            moving_avg = np.convolve(self.episode_rewards, np.ones(window)/window, mode='valid')\n",
        "            axes[0, 0].plot(range(window-1, len(self.episode_rewards)), moving_avg,\n",
        "                           label=f'Moving Average ({window})', linewidth=2)\n",
        "\n",
        "        axes[0, 0].set_xlabel('Episode')\n",
        "        axes[0, 0].set_ylabel('Reward')\n",
        "        axes[0, 0].set_title('DQN Episode Rewards')\n",
        "        axes[0, 0].legend()\n",
        "        axes[0, 0].grid(True, alpha=0.3)\n",
        "\n",
        "        # Training losses\n",
        "        if self.losses:\n",
        "            axes[0, 1].plot(self.losses, alpha=0.3, label='Loss')\n",
        "\n",
        "            if len(self.losses) > 50:\n",
        "                loss_ma = np.convolve(self.losses, np.ones(50)/50, mode='valid')\n",
        "                axes[0, 1].plot(range(49, len(self.losses)), loss_ma,\n",
        "                               label='Moving Average (50)', linewidth=2)\n",
        "\n",
        "            axes[0, 1].set_xlabel('Training Step')\n",
        "            axes[0, 1].set_ylabel('Loss')\n",
        "            axes[0, 1].set_title('DQN Training Loss')\n",
        "            axes[0, 1].legend()\n",
        "            axes[0, 1].grid(True, alpha=0.3)\n",
        "\n",
        "        # Epsilon decay\n",
        "        epsilon_history = []\n",
        "        eps = 1.0\n",
        "        for i in range(len(self.episode_rewards)):\n",
        "            epsilon_history.append(eps)\n",
        "            if eps > self.min_epsilon:\n",
        "                eps *= self.epsilon_decay\n",
        "\n",
        "        axes[1, 0].plot(epsilon_history)\n",
        "        axes[1, 0].set_xlabel('Episode')\n",
        "        axes[1, 0].set_ylabel('Epsilon')\n",
        "        axes[1, 0].set_title('Exploration Rate Decay')\n",
        "        axes[1, 0].grid(True, alpha=0.3)\n",
        "\n",
        "        # Q-value visualization untuk center state\n",
        "        center_state = (2, 2)  # Middle of 5x5 grid\n",
        "        if hasattr(env, 'width') and hasattr(env, 'height'):\n",
        "            center_vector = self.state_to_vector(center_state, env)\n",
        "            q_values = self.q_network.predict(center_vector.reshape(1, -1), verbose=0)[0]\n",
        "\n",
        "            actions = ['Up', 'Down', 'Left', 'Right']\n",
        "            axes[1, 1].bar(actions, q_values)\n",
        "            axes[1, 1].set_ylabel('Q-Value')\n",
        "            axes[1, 1].set_title(f'Q-Values for State {center_state}')\n",
        "            axes[1, 1].grid(True, alpha=0.3)\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "# Train DQN agent\n",
        "print(\"=\"*60)\n",
        "print(\"DEEP Q-NETWORK DEMONSTRATION\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "env = GridWorld(width=5, height=5)\n",
        "state_size = env.width * env.height + 3  # Position encoding + distance features\n",
        "\n",
        "dqn_agent = DQN(state_size=state_size, action_size=4,\n",
        "                learning_rate=0.001, gamma=0.99,\n",
        "                epsilon=1.0, epsilon_decay=0.995,\n",
        "                memory_size=10000, batch_size=32)\n",
        "\n",
        "# Train DQN\n",
        "dqn_agent.train(env, num_episodes=800, max_steps_per_episode=100)\n",
        "\n",
        "# Test DQN\n",
        "print(\"\\nTesting DQN agent:\")\n",
        "dqn_rewards = dqn_agent.test(env, num_episodes=5, render=True)\n",
        "\n",
        "# Plot results\n",
        "dqn_agent.plot_training_progress()"
      ],
      "metadata": {
        "id": "VHin8sM6tElN"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}