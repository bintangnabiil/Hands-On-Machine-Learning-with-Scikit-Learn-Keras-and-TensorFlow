{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNlI3rokNythSLKffPIBje0",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bintangnabiil/Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow/blob/main/Rangkuman_Chapter_16.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Chapter 16: Natural Language Processing with RNNs and Attention\n",
        "Chapter ini membahas penerapan Recurrent Neural Networks (RNNs) dan mekanisme Attention untuk Natural Language Processing (NLP). Chapter ini merupakan kelanjutan dari Chapter 15 yang fokus pada aplikasi khusus untuk memproses bahasa natural.\n",
        "\n",
        "Natural Language Processing adalah cabang dari artificial intelligence yang berfokus pada interaksi antara komputer dan bahasa manusia. Tantangan utama dalam NLP adalah bahasa manusia memiliki karakteristik yang kompleks seperti ambiguitas, konteks yang berubah, idiom, dan struktur gramatikal yang beragam. RNN dan mekanisme attention menjadi solusi yang efektif karena mampu menangkap dependensi temporal dan konteks yang panjang dalam teks.\n",
        "<br><br>\n",
        "\n",
        "###Perbedaan utama antara pemrosesan teks dan data sekuensial lainnya adalah:\n",
        "\n",
        "- Diskretisasi: Teks terdiri dari token diskret (kata, karakter) bukan nilai kontinu\n",
        "- Variabilitas panjang: Kalimat memiliki panjang yang sangat bervariasi\n",
        "- Konteks semantik: Makna kata bergantung pada konteks sekitarnya\n",
        "- Struktur hierarkis: Bahasa memiliki struktur dari karakter → kata → frasa → kalimat → paragraf\n",
        "\n",
        "##1. Character-Level Language Model\n",
        "Language model karakter-level memprediksi karakter berikutnya dalam sebuah teks berdasarkan karakter-karakter sebelumnya. Ini adalah pendekatan dasar dalam text generation.\n",
        "<br><br>\n",
        "\n",
        "###Character-level language model memiliki beberapa keuntungan:\n",
        "\n",
        "- Vocabulary terbatas: Hanya perlu menangani ~100 karakter unik\n",
        "- No OOV problem: Tidak ada masalah Out-of-Vocabulary karena semua kata dapat dibentuk dari karakter\n",
        "- Multilingual: Dapat menangani berbagai bahasa dengan arsitektur yang sama\n",
        "- Morfologi: Dapat mempelajari morphological patterns secara implisit\n",
        "<br><br>\n",
        "\n",
        "###Tantangan Character-Level:\n",
        "\n",
        "- Long sequences: Teks menjadi sangat panjang dalam representasi karakter\n",
        "- Long-term dependencies: Sulit menangkap dependensi antar kata yang berjauhan\n",
        "- Computational cost: Membutuhkan komputasi yang lebih besar\n",
        "- Slower convergence: Training membutuhkan waktu lebih lama\n",
        "<br><br>\n",
        "\n",
        "###Prinsip Kerja:\n",
        "Model character-level bekerja dengan prinsip autoregressive, di mana prediksi karakter ke-t bergantung pada semua karakter sebelumnya: P(c_t | c_1, c_2, ..., c_{t-1}). Dalam praktiknya, RNN mempertahankan hidden state yang merangkum informasi dari karakter-karakter sebelumnya.\n",
        "<br><br>\n",
        "\n",
        "###Aplikasi:\n",
        "\n",
        "- Text generation (seperti GPT awal)\n",
        "- Text completion\n",
        "- Style transfer\n",
        "- Language modeling untuk compression"
      ],
      "metadata": {
        "id": "u-kdvWG5G_4M"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Implementasi Character-Level RNN"
      ],
      "metadata": {
        "id": "P0KWND2fHMDw"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PNVqNEK1GeVs"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import numpy as np\n",
        "import requests\n",
        "\n",
        "# Download dataset (contoh: Shakespeare text)\n",
        "shakespeare_url = \"https://homl.info/shakespeare\"\n",
        "filepath = keras.utils.get_file(\"shakespeare.txt\", shakespeare_url)\n",
        "with open(filepath) as f:\n",
        "    shakespeare_text = f.read()\n",
        "\n",
        "print(f\"Length of text: {len(shakespeare_text)} characters\")\n",
        "print(shakespeare_text[:100])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Preprocessing Text Data"
      ],
      "metadata": {
        "id": "8S6k8IwMHOp5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Buat vocabulary dan encoding\n",
        "text = shakespeare_text\n",
        "vocab = sorted(set(text))\n",
        "print(f\"Unique characters: {len(vocab)}\")\n",
        "\n",
        "# Character to integer mapping\n",
        "char_to_ids = {char: i for i, char in enumerate(vocab)}\n",
        "ids_to_char = {i: char for i, char in enumerate(vocab)}\n",
        "\n",
        "# Convert text to integer sequences\n",
        "encoded = np.array([char_to_ids[char] for char in text])\n",
        "\n",
        "# Create training sequences\n",
        "seq_length = 100\n",
        "examples_per_epoch = len(text) // seq_length\n",
        "\n",
        "# Convert to tf.data.Dataset\n",
        "char_dataset = tf.data.Dataset.from_tensor_slices(encoded)\n",
        "\n",
        "# Batch the characters into sequences\n",
        "sequences = char_dataset.batch(seq_length + 1, drop_remainder=True)\n",
        "\n",
        "def split_input_target(chunk):\n",
        "    input_text = chunk[:-1]\n",
        "    target_text = chunk[1:]\n",
        "    return input_text, target_text\n",
        "\n",
        "# Create input-target pairs\n",
        "dataset = sequences.map(split_input_target)\n",
        "\n",
        "# Shuffle and batch\n",
        "BATCH_SIZE = 64\n",
        "BUFFER_SIZE = 10000\n",
        "\n",
        "dataset = (dataset\n",
        "          .shuffle(BUFFER_SIZE)\n",
        "          .batch(BATCH_SIZE, drop_remainder=True)\n",
        "          .prefetch(tf.data.experimental.AUTOTUNE))\n",
        "\n",
        "# Contoh data\n",
        "for input_example, target_example in dataset.take(1):\n",
        "    print(f'Input shape: {input_example.shape}')\n",
        "    print(f'Target shape: {target_example.shape}')"
      ],
      "metadata": {
        "id": "GXnQJyXHHQMR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Building Character-Level Model"
      ],
      "metadata": {
        "id": "htN_E4IYHR51"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def build_char_rnn_model(vocab_size, embedding_dim=256, rnn_units=1024, batch_size=None):\n",
        "    model = keras.Sequential([\n",
        "        keras.layers.Embedding(vocab_size, embedding_dim,\n",
        "                              batch_input_shape=[batch_size, None]),\n",
        "        keras.layers.GRU(rnn_units,\n",
        "                        return_sequences=True,\n",
        "                        stateful=True,\n",
        "                        recurrent_initializer='glorot_uniform'),\n",
        "        keras.layers.Dense(vocab_size)\n",
        "    ])\n",
        "    return model\n",
        "\n",
        "# Build and compile model\n",
        "vocab_size = len(vocab)\n",
        "model = build_char_rnn_model(vocab_size, batch_size=BATCH_SIZE)\n",
        "\n",
        "# Define loss function\n",
        "def loss(labels, logits):\n",
        "    return tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True)\n",
        "\n",
        "model.compile(optimizer='adam', loss=loss)\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "SvMYK17CHTir"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Training the Character Model"
      ],
      "metadata": {
        "id": "taTaxDPPHVLV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Configure checkpoints\n",
        "checkpoint_dir = './training_checkpoints'\n",
        "checkpoint_prefix = checkpoint_dir + \"/ckpt_{epoch}\"\n",
        "\n",
        "checkpoint_callback = keras.callbacks.ModelCheckpoint(\n",
        "    filepath=checkpoint_prefix,\n",
        "    save_weights_only=True)\n",
        "\n",
        "# Train the model\n",
        "EPOCHS = 10\n",
        "history = model.fit(dataset,\n",
        "                   epochs=EPOCHS,\n",
        "                   callbacks=[checkpoint_callback])"
      ],
      "metadata": {
        "id": "ZAGuDdRrHWWr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Text Generation"
      ],
      "metadata": {
        "id": "b_rWrox1HX1_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Build model for generation (batch_size=1)\n",
        "generation_model = build_char_rnn_model(vocab_size, batch_size=1)\n",
        "generation_model.load_weights(tf.train.latest_checkpoint(checkpoint_dir))\n",
        "generation_model.build(tf.TensorShape([1, None]))\n",
        "\n",
        "def generate_text(model, start_string, generation_length=1000, temperature=1.0):\n",
        "    # Convert start string to numbers\n",
        "    input_eval = [char_to_ids[s] for s in start_string]\n",
        "    input_eval = tf.expand_dims(input_eval, 0)\n",
        "\n",
        "    text_generated = []\n",
        "    model.reset_states()\n",
        "\n",
        "    for i in range(generation_length):\n",
        "        predictions = model(input_eval)\n",
        "        predictions = tf.squeeze(predictions, 0)\n",
        "\n",
        "        # Apply temperature\n",
        "        predictions = predictions / temperature\n",
        "        predicted_id = tf.random.categorical(predictions, num_samples=1)[-1,0].numpy()\n",
        "\n",
        "        # Pass predicted character as input for next time step\n",
        "        input_eval = tf.expand_dims([predicted_id], 0)\n",
        "\n",
        "        text_generated.append(ids_to_char[predicted_id])\n",
        "\n",
        "    return start_string + ''.join(text_generated)\n",
        "\n",
        "# Generate text\n",
        "print(generate_text(generation_model, start_string=\"ROMEO: \", temperature=0.5))"
      ],
      "metadata": {
        "id": "rvtJbQf4HY2I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##2. Sentiment Analysis dengan LSTM"
      ],
      "metadata": {
        "id": "XKqiSmy2HajO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Teori Sentiment Analysis\n",
        "Sentiment analysis atau opinion mining adalah tugas klasifikasi teks yang bertujuan menentukan polaritas emosi dalam teks (positif, negatif, atau netral). Ini adalah salah satu aplikasi NLP yang paling populer dengan berbagai use case komersial.\n",
        "<br><br>\n",
        "\n",
        "###Tantangan dalam Sentiment Analysis:\n",
        "\n",
        "- Konteks: Kata yang sama dapat memiliki sentimen berbeda dalam konteks berbeda\n",
        "- Sarkasme dan ironi: Sulit dideteksi oleh model\n",
        "- Negasi: Kata negasi dapat membalik sentimen keseluruhan\n",
        "- Intensitas: Perbedaan antara \"good\" dan \"excellent\"\n",
        "- Domain dependence: Model yang trained pada review film mungkin tidak baik untuk review produk\n",
        "<br><br>\n",
        "\n",
        "###Mengapa LSTM Efektif untuk Sentiment Analysis:\n",
        "\n",
        "- Sequential processing: Dapat memproses kata secara berurutan\n",
        "- Long-term memory: Mengingat konteks dari awal kalimat\n",
        "- Gating mechanism: Dapat memfilter informasi yang tidak relevan\n",
        "- Bidirectional capability: Dapat mempertimbangkan konteks masa depan\n",
        "<br><br>\n",
        "\n",
        "###Arsitektur Umum:\n",
        "\n",
        "- Embedding layer: Mengkonversi kata menjadi dense vector\n",
        "- LSTM layer: Memproses sequence dan menangkap dependencies\n",
        "- Dense layer: Klasifikasi final dengan sigmoid/softmax activation\n",
        "- Dropout: Regularization untuk mencegah overfitting\n",
        "<br><br>\n",
        "\n",
        "###Preprocessing Khusus untuk Sentiment:\n",
        "\n",
        "- Handling negations: Menggabungkan kata negasi dengan kata berikutnya\n",
        "- Emoticons: Mengkonversi emoticon menjadi kata\n",
        "- Normalization: Mengubah repeated characters (gooood → good)\n",
        "- Stop words: Biasanya tidak dihilangkan karena dapat mempengaruhi sentimen"
      ],
      "metadata": {
        "id": "MRL3Vd99M_Ks"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Persiapan Data untuk Sentiment Analysis"
      ],
      "metadata": {
        "id": "tOk9v5xnHcUb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Menggunakan IMDB dataset\n",
        "from tensorflow.keras.datasets import imdb\n",
        "from tensorflow.keras.preprocessing import sequence\n",
        "\n",
        "# Load IMDB dataset\n",
        "max_features = 10000  # Only consider top 10k words\n",
        "maxlen = 500  # Cut texts after this number of words\n",
        "\n",
        "print('Loading data...')\n",
        "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features)\n",
        "print(f'Train sequences: {len(x_train)}')\n",
        "print(f'Test sequences: {len(x_test)}')\n",
        "\n",
        "# Pad sequences\n",
        "print('Pad sequences (samples x time)')\n",
        "x_train = sequence.pad_sequences(x_train, maxlen=maxlen)\n",
        "x_test = sequence.pad_sequences(x_test, maxlen=maxlen)\n",
        "print(f'x_train shape: {x_train.shape}')\n",
        "print(f'x_test shape: {x_test.shape}')"
      ],
      "metadata": {
        "id": "D_E3liKRHlv1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###LSTM Model for Sentiment Analysis"
      ],
      "metadata": {
        "id": "NC1GnaKDHnmb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def build_sentiment_model(max_features, maxlen):\n",
        "    model = keras.Sequential([\n",
        "        keras.layers.Embedding(max_features, 128),\n",
        "        keras.layers.LSTM(128, dropout=0.2, recurrent_dropout=0.2),\n",
        "        keras.layers.Dense(1, activation='sigmoid')\n",
        "    ])\n",
        "\n",
        "    model.compile(loss='binary_crossentropy',\n",
        "                 optimizer='adam',\n",
        "                 metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "sentiment_model = build_sentiment_model(max_features, maxlen)\n",
        "sentiment_model.summary()\n",
        "\n",
        "# Train model\n",
        "print('Training...')\n",
        "history = sentiment_model.fit(x_train, y_train,\n",
        "                             batch_size=32,\n",
        "                             epochs=5,\n",
        "                             validation_data=(x_test, y_test),\n",
        "                             verbose=1)\n",
        "\n",
        "# Evaluate\n",
        "score, acc = sentiment_model.evaluate(x_test, y_test, batch_size=32, verbose=0)\n",
        "print(f'Test accuracy: {acc:.4f}')"
      ],
      "metadata": {
        "id": "jjyee3I2Hp1J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##3. Encoder-Decoder untuk Neural Machine Translation\n",
        "\n",
        "###Teori Neural Machine Translation (NMT)\n",
        "Neural Machine Translation adalah pendekatan end-to-end untuk machine translation menggunakan neural networks. Berbeda dengan statistical machine translation yang menggunakan multiple components terpisah, NMT menggunakan single neural network yang dilatih secara end-to-end.\n",
        "<br><br>\n",
        "\n",
        "###Evolusi Machine Translation:\n",
        "\n",
        "- Rule-based MT: Menggunakan linguistic rules dan dictionaries\n",
        "- Statistical MT: Menggunakan statistical models dari parallel corpora\n",
        "- Neural MT: Menggunakan neural networks, khususnya sequence-to-sequence models\n",
        "<br><br>\n",
        "\n",
        "###Keuntungan NMT:\n",
        "\n",
        "- End-to-end training: Tidak memerlukan alignment atau phrase extraction\n",
        "- Better fluency: Menghasilkan terjemahan yang lebih natural\n",
        "- Contextual understanding: Lebih baik dalam menangkap konteks\n",
        "- Rare word handling: Lebih baik dalam menangani kata-kata jarang\n",
        "\n",
        "Arsitektur Sequence-to-Sequence:\n",
        "Seq2Seq model terdiri dari dua komponen utama:\n",
        "<br><br>\n",
        "\n",
        "###Encoder:\n",
        "\n",
        "- Membaca input sequence dan mengkodekannya menjadi fixed-size context vector\n",
        "- Biasanya menggunakan LSTM/GRU bidirectional\n",
        "- Context vector adalah hidden state terakhir dari encoder\n",
        "- Bertugas memampatkan seluruh informasi input ke dalam single vector\n",
        "\n",
        "\n",
        "###Decoder:\n",
        "\n",
        "- Menghasilkan output sequence dari context vector\n",
        "- Menggunakan context vector sebagai initial hidden state\n",
        "- Bekerja secara autoregressive (prediksi token berikutnya berdasarkan token sebelumnya)\n",
        "- Training menggunakan teacher forcing untuk mempercepat konvergensi\n",
        "<br><br>\n",
        "\n",
        "###Masalah Information Bottleneck:\n",
        "Arsitektur encoder-decoder basic memiliki masalah fundamental yaitu information bottleneck. Seluruh informasi dari input sequence harus dikompres ke dalam single context vector dengan ukuran tetap. Ini menyebabkan:\n",
        "\n",
        "- Loss of information untuk sequence yang panjang\n",
        "- Degradasi performance pada kalimat yang panjang\n",
        "- Kesulitan dalam menangkap detail-detail penting\n",
        "<br><br>\n",
        "\n",
        "###Teacher Forcing:\n",
        "Teknik training di mana decoder menggunakan ground truth dari timestep sebelumnya sebagai input, bukan prediksi model sendiri. Ini mempercepat training tetapi dapat menyebabkan exposure bias saat inference."
      ],
      "metadata": {
        "id": "H_EV7jVrHtIg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Sequence-to-Sequence Architecture"
      ],
      "metadata": {
        "id": "6ijPGvasHu5O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Contoh implementasi Encoder-Decoder untuk terjemahan\n",
        "def build_encoder_decoder_model(input_vocab_size, target_vocab_size,\n",
        "                               embedding_dim=256, latent_dim=512):\n",
        "\n",
        "    # Encoder\n",
        "    encoder_inputs = keras.Input(shape=(None,))\n",
        "    encoder_embedding = keras.layers.Embedding(input_vocab_size, embedding_dim)(encoder_inputs)\n",
        "    encoder_lstm = keras.layers.LSTM(latent_dim, return_state=True)\n",
        "    encoder_outputs, state_h, state_c = encoder_lstm(encoder_embedding)\n",
        "    encoder_states = [state_h, state_c]\n",
        "\n",
        "    # Decoder\n",
        "    decoder_inputs = keras.Input(shape=(None,))\n",
        "    decoder_embedding = keras.layers.Embedding(target_vocab_size, embedding_dim)\n",
        "    decoder_embedding_output = decoder_embedding(decoder_inputs)\n",
        "    decoder_lstm = keras.layers.LSTM(latent_dim, return_sequences=True, return_state=True)\n",
        "    decoder_outputs, _, _ = decoder_lstm(decoder_embedding_output, initial_state=encoder_states)\n",
        "    decoder_dense = keras.layers.Dense(target_vocab_size, activation='softmax')\n",
        "    decoder_outputs = decoder_dense(decoder_outputs)\n",
        "\n",
        "    # Define the model\n",
        "    model = keras.Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
        "    return model\n",
        "\n",
        "# Contoh penggunaan (perlu data preprocessing yang sesuai)\n",
        "# model = build_encoder_decoder_model(input_vocab_size=10000, target_vocab_size=8000)\n",
        "# model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "HYLC7pMXHwFt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##4. Attention Mechanism\n",
        "###Teori Attention Mechanism\n",
        "Attention mechanism adalah teknik yang memungkinkan model untuk \"memperhatikan\" bagian-bagian berbeda dari input sequence ketika menghasilkan setiap token output. Ini mengatasi masalah information bottleneck pada arsitektur encoder-decoder tradisional.\n",
        "<br><br>\n",
        "\n",
        "###Motivasi Attention:\n",
        "Dalam terjemahan manusia, kita tidak mengingat seluruh kalimat input lalu menerjemahkannya. Sebaliknya, kita fokus pada bagian-bagian tertentu dari input ketika menerjemahkan setiap kata. Attention mechanism meniru proses ini.\n",
        "<br><br>\n",
        "\n",
        "###Prinsip Kerja Attention:\n",
        "\n",
        "- Alignment: Menentukan bagian mana dari input yang relevan untuk output saat ini\n",
        "- Weighting: Memberikan bobot pada setiap bagian input berdasarkan relevansinya\n",
        "- Context: Membuat context vector berdasarkan weighted sum dari encoder outputs\n",
        "<br><br>\n",
        "\n",
        "###Jenis-jenis Attention:\n",
        "\n",
        "###a. Bahdanau Attention (Additive Attention):\n",
        "\n",
        "- Menggunakan feedforward network untuk menghitung alignment scores\n",
        "- Score(h_i, s_j) = v^T tanh(W_h h_i + W_s s_j)\n",
        "- Lebih fleksibel tetapi lebih lambat\n",
        "<br><br>\n",
        "\n",
        "###b. Luong Attention (Multiplicative Attention):\n",
        "\n",
        "- Menggunakan dot product untuk menghitung alignment scores\n",
        "- Score(h_i, s_j) = h_i^T W s_j (general) atau h_i^T s_j (dot)\n",
        "- Lebih efisien tetapi kurang fleksibel\n",
        "<br><br>\n",
        "\n",
        "###c. Self-Attention:\n",
        "\n",
        "- Attention dalam satu sequence (input dengan dirinya sendiri)\n",
        "- Memungkinkan model memahami relationship antar posisi dalam sequence\n",
        "- Basis dari Transformer architecture\n",
        "<br><br>\n",
        "\n",
        "###Keuntungan Attention:\n",
        "\n",
        "- Mengatasi bottleneck: Tidak perlu kompres semua informasi ke single vector\n",
        "- Better long sequences: Performa tidak menurun drastis pada sequence panjang\n",
        "- Interpretability: Attention weights memberikan insight tentang alignment\n",
        "- Selective focus: Model dapat fokus pada informasi yang relevan\n",
        "<br><br>\n",
        "\n",
        "###Attention Score Computation:\n",
        "\n",
        "- Compute alignment scores: e_ij = align(s_{i-1}, h_j)\n",
        "- Normalize scores: α_ij = softmax(e_ij)\n",
        "- Compute context: c_i = Σ α_ij * h_j\n",
        "<br><br>\n",
        "\n",
        "###Multi-Head Attention:\n",
        "Ekstensi dari attention mechanism yang memungkinkan model untuk memperhatikan informasi dari berbagai representasi subspaces secara bersamaan. Setiap \"head\" mempelajari jenis relationship yang berbeda."
      ],
      "metadata": {
        "id": "Qg5VXNhrHx4k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Bahdanau Attention"
      ],
      "metadata": {
        "id": "l7Rg5ZsTHznP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class BahdanauAttention(keras.layers.Layer):\n",
        "    def __init__(self, units):\n",
        "        super(BahdanauAttention, self).__init__()\n",
        "        self.W1 = keras.layers.Dense(units)\n",
        "        self.W2 = keras.layers.Dense(units)\n",
        "        self.V = keras.layers.Dense(1)\n",
        "\n",
        "    def call(self, query, values):\n",
        "        # query shape: (batch_size, hidden_size)\n",
        "        # values shape: (batch_size, max_len, hidden_size)\n",
        "\n",
        "        # Expand query to (batch_size, 1, hidden_size)\n",
        "        query_with_time_axis = tf.expand_dims(query, 1)\n",
        "\n",
        "        # Calculate attention weights\n",
        "        score = self.V(tf.nn.tanh(\n",
        "            self.W1(query_with_time_axis) + self.W2(values)))\n",
        "\n",
        "        # Attention weights shape: (batch_size, max_len, 1)\n",
        "        attention_weights = tf.nn.softmax(score, axis=1)\n",
        "\n",
        "        # Context vector shape: (batch_size, hidden_size)\n",
        "        context_vector = attention_weights * values\n",
        "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
        "\n",
        "        return context_vector, attention_weights\n",
        "\n",
        "# Encoder dengan Attention\n",
        "class Encoder(keras.Model):\n",
        "    def __init__(self, vocab_size, embedding_dim, enc_units, batch_sz):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.batch_sz = batch_sz\n",
        "        self.enc_units = enc_units\n",
        "        self.embedding = keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "        self.gru = keras.layers.GRU(self.enc_units,\n",
        "                                   return_sequences=True,\n",
        "                                   return_state=True,\n",
        "                                   recurrent_initializer='glorot_uniform')\n",
        "\n",
        "    def call(self, x, hidden):\n",
        "        x = self.embedding(x)\n",
        "        output, state = self.gru(x, initial_state=hidden)\n",
        "        return output, state\n",
        "\n",
        "    def initialize_hidden_state(self):\n",
        "        return tf.zeros((self.batch_sz, self.enc_units))\n",
        "\n",
        "# Decoder dengan Attention\n",
        "class Decoder(keras.Model):\n",
        "    def __init__(self, vocab_size, embedding_dim, dec_units, batch_sz):\n",
        "        super(Decoder, self).__init__()\n",
        "        self.batch_sz = batch_sz\n",
        "        self.dec_units = dec_units\n",
        "        self.embedding = keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "        self.gru = keras.layers.GRU(self.dec_units,\n",
        "                                   return_sequences=True,\n",
        "                                   return_state=True,\n",
        "                                   recurrent_initializer='glorot_uniform')\n",
        "        self.fc = keras.layers.Dense(vocab_size)\n",
        "\n",
        "        self.attention = BahdanauAttention(self.dec_units)\n",
        "\n",
        "    def call(self, x, hidden, enc_output):\n",
        "        context_vector, attention_weights = self.attention(hidden, enc_output)\n",
        "\n",
        "        x = self.embedding(x)\n",
        "        x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)\n",
        "\n",
        "        output, state = self.gru(x)\n",
        "        output = tf.reshape(output, (-1, output.shape[2]))\n",
        "\n",
        "        x = self.fc(output)\n",
        "\n",
        "        return x, state, attention_weights"
      ],
      "metadata": {
        "id": "Kb2-f9HCH1Ap"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Training Loop dengan Attention"
      ],
      "metadata": {
        "id": "MrqAbtYBH3qt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_step(inp, targ, enc_hidden, encoder, decoder, optimizer,\n",
        "               loss_object, batch_size, targ_lang_tokenizer):\n",
        "    loss = 0\n",
        "\n",
        "    with tf.GradientTape() as tape:\n",
        "        enc_output, enc_hidden = encoder(inp, enc_hidden)\n",
        "\n",
        "        dec_hidden = enc_hidden\n",
        "        dec_input = tf.expand_dims([targ_lang_tokenizer.word_index['<start>']] * batch_size, 1)\n",
        "\n",
        "        # Teacher forcing\n",
        "        for t in range(1, targ.shape[1]):\n",
        "            predictions, dec_hidden, _ = decoder(dec_input, dec_hidden, enc_output)\n",
        "            loss += loss_object(targ[:, t], predictions)\n",
        "            dec_input = tf.expand_dims(targ[:, t], 1)  # Teacher forcing\n",
        "\n",
        "    batch_loss = (loss / int(targ.shape[1]))\n",
        "    variables = encoder.trainable_variables + decoder.trainable_variables\n",
        "    gradients = tape.gradient(loss, variables)\n",
        "    optimizer.apply_gradients(zip(gradients, variables))\n",
        "\n",
        "    return batch_loss\n",
        "\n",
        "# Fungsi untuk training\n",
        "def train_model(dataset, encoder, decoder, epochs, steps_per_epoch):\n",
        "    optimizer = tf.keras.optimizers.Adam()\n",
        "    loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction='none')\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        enc_hidden = encoder.initialize_hidden_state()\n",
        "        total_loss = 0\n",
        "\n",
        "        for (batch, (inp, targ)) in enumerate(dataset.take(steps_per_epoch)):\n",
        "            batch_loss = train_step(inp, targ, enc_hidden, encoder, decoder,\n",
        "                                  optimizer, loss_object, BATCH_SIZE, target_tokenizer)\n",
        "            total_loss += batch_loss\n",
        "\n",
        "            if batch % 100 == 0:\n",
        "                print(f'Epoch {epoch+1} Batch {batch} Loss {batch_loss.numpy():.4f}')\n",
        "\n",
        "        print(f'Epoch {epoch+1} Loss {total_loss/steps_per_epoch:.4f}')"
      ],
      "metadata": {
        "id": "b6VdsZe7H5cb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##5. Transformer Architecture (Preview)\n",
        "###Teori Transformer\n",
        "Transformer adalah arsitektur yang sepenuhnya bergantung pada attention mechanism tanpa menggunakan RNN atau CNN. Diperkenalkan dalam paper \"Attention Is All You Need\" (Vaswani et al., 2017), Transformer menjadi foundation untuk model-model modern seperti BERT, GPT, dan T5.\n",
        "<br><br>\n",
        "\n",
        "###Motivasi Transformer:\n",
        "\n",
        "- Parallelization: RNN harus diproses secara sequential, sedangkan Transformer dapat diparalelkan\n",
        "- Long-range dependencies: Self-attention dapat menangkap dependensi jangka panjang lebih baik\n",
        "- Computational efficiency: Lebih efisien untuk sequence yang panjang\n",
        "- Training speed: Dapat dilatih lebih cepat karena paralelisasi\n",
        "<br><br>\n",
        "\n",
        "###Komponen Utama Transformer:\n",
        "\n",
        "###a. Self-Attention Mechanism:\n",
        "\n",
        "- Query (Q), Key (K), Value (V) matrices\n",
        "- Attention(Q,K,V) = softmax(QK^T/√d_k)V\n",
        "- Memungkinkan setiap posisi memperhatikan semua posisi lain dalam sequence\n",
        "<br><br>\n",
        "\n",
        "###b. Multi-Head Attention:\n",
        "\n",
        "- Multiple attention heads yang belajar representation yang berbeda\n",
        "- Hasil dari semua heads digabungkan dan diproyeksikan\n",
        "- Memungkinkan model fokus pada berbagai aspek relationship\n",
        "<br><br>\n",
        "\n",
        "###c. Position Encoding:\n",
        "\n",
        "- Karena tidak ada urutan inherent dalam attention, perlu positional information\n",
        "- Menggunakan sinusoidal encoding atau learned positional embeddings\n",
        "- PE(pos, 2i) = sin(pos/10000^(2i/d_model))\n",
        "- PE(pos, 2i+1) = cos(pos/10000^(2i/d_model))\n",
        "<br><br>\n",
        "\n",
        "###d. Feed-Forward Networks:\n",
        "\n",
        "- Two-layer neural network dengan ReLU activation\n",
        "- FFN(x) = max(0, xW1 + b1)W2 + b2\n",
        "- Applied pada setiap posisi secara terpisah\n",
        "<br><br>\n",
        "\n",
        "###e. Residual Connections dan Layer Normalization:\n",
        "\n",
        "- Residual connection: output = LayerNorm(x + Sublayer(x))\n",
        "- Membantu training pada network yang dalam\n",
        "- Layer normalization lebih stabil untuk sequence modeling\n",
        "<br><br>\n",
        "\n",
        "###Encoder-Decoder Structure:\n",
        "\n",
        "- Encoder: Stack of identical layers, each with multi-head self-attention dan FFN\n",
        "- Decoder: Stack dengan masked multi-head attention, encoder-decoder attention, dan FFN\n",
        "- Masking: Mencegah decoder melihat future tokens selama training\n",
        "<br><br>\n",
        "\n",
        "###Keuntungan Transformer:\n",
        "\n",
        "- Parallelizable: Semua posisi dapat diproses bersamaan\n",
        "- Long-range dependencies: Self-attention menghubungkan posisi yang jauh\n",
        "- Interpretable: Attention weights memberikan insight yang jelas\n",
        "- Transfer learning: Pre-trained model dapat di-fine-tune untuk berbagai tugas\n",
        "<br><br>\n",
        "\n",
        "###Computational Complexity:\n",
        "\n",
        "- Self-attention: O(n²d) dimana n adalah sequence length\n",
        "- RNN: O(nd²) tetapi sequential\n",
        "- Untuk sequence pendek dengan dimensi besar, RNN lebih efisien\n",
        "- Untuk sequence panjang, Transformer lebih efisien dengan paralelisasi"
      ],
      "metadata": {
        "id": "LOPTS-f-H63X"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Self-Attention Mechanism"
      ],
      "metadata": {
        "id": "Uwr7_eF4H8yC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def scaled_dot_product_attention(q, k, v, mask=None):\n",
        "    \"\"\"Calculate the attention weights.\"\"\"\n",
        "    matmul_qk = tf.matmul(q, k, transpose_b=True)\n",
        "\n",
        "    # Scale matmul_qk\n",
        "    dk = tf.cast(tf.shape(k)[-1], tf.float32)\n",
        "    scaled_attention_logits = matmul_qk / tf.math.sqrt(dk)\n",
        "\n",
        "    # Add mask to the scaled tensor\n",
        "    if mask is not None:\n",
        "        scaled_attention_logits += (mask * -1e9)\n",
        "\n",
        "    # Softmax\n",
        "    attention_weights = tf.nn.softmax(scaled_attention_logits, axis=-1)\n",
        "    output = tf.matmul(attention_weights, v)\n",
        "\n",
        "    return output, attention_weights\n",
        "\n",
        "class MultiHeadAttention(keras.layers.Layer):\n",
        "    def __init__(self, d_model, num_heads):\n",
        "        super(MultiHeadAttention, self).__init__()\n",
        "        self.num_heads = num_heads\n",
        "        self.d_model = d_model\n",
        "\n",
        "        assert d_model % self.num_heads == 0\n",
        "\n",
        "        self.depth = d_model // self.num_heads\n",
        "\n",
        "        self.wq = keras.layers.Dense(d_model)\n",
        "        self.wk = keras.layers.Dense(d_model)\n",
        "        self.wv = keras.layers.Dense(d_model)\n",
        "\n",
        "        self.dense = keras.layers.Dense(d_model)\n",
        "\n",
        "    def split_heads(self, x, batch_size):\n",
        "        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.depth))\n",
        "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
        "\n",
        "    def call(self, v, k, q, mask=None):\n",
        "        batch_size = tf.shape(q)[0]\n",
        "\n",
        "        q = self.wq(q)\n",
        "        k = self.wk(k)\n",
        "        v = self.wv(v)\n",
        "\n",
        "        q = self.split_heads(q, batch_size)\n",
        "        k = self.split_heads(k, batch_size)\n",
        "        v = self.split_heads(v, batch_size)\n",
        "\n",
        "        scaled_attention, attention_weights = scaled_dot_product_attention(q, k, v, mask)\n",
        "\n",
        "        scaled_attention = tf.transpose(scaled_attention, perm=[0, 2, 1, 3])\n",
        "        concat_attention = tf.reshape(scaled_attention, (batch_size, -1, self.d_model))\n",
        "\n",
        "        output = self.dense(concat_attention)\n",
        "\n",
        "        return output, attention_weights"
      ],
      "metadata": {
        "id": "fVKe0eJNH9P6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##6. Text Preprocessing dan Tokenization\n",
        "###Teori Text Preprocessing\n",
        "Text preprocessing adalah langkah kritis dalam NLP yang menentukan kualitas input untuk model. Preprocessing yang baik dapat meningkatkan performa model secara signifikan, sementara preprocessing yang buruk dapat merusak informasi penting.\n",
        "<br><br>\n",
        "\n",
        "###Prinsip Dasar Preprocessing:\n",
        "\n",
        "- Normalization: Menstandarkan format teks\n",
        "- Tokenization: Memecah teks menjadi unit-unit yang dapat diproses\n",
        "- Encoding: Mengkonversi token menjadi representasi numerik\n",
        "- Sequence preparation: Menyiapkan sequence untuk model\n",
        "<br><br>\n",
        "\n",
        "###Tahapan Preprocessing:\n",
        "\n",
        "###a. Text Cleaning:\n",
        "\n",
        "- Case normalization: Lowercase/uppercase standardization\n",
        "- Punctuation handling: Removal atau normalization\n",
        "- Number handling: Replacement dengan placeholders\n",
        "- Special characters: Removal atau normalization\n",
        "- Unicode normalization: Menangani berbagai encoding\n",
        "<br><br>\n",
        "\n",
        "###b. Tokenization Strategies:\n",
        "\n",
        "- Word-level: Memecah berdasarkan whitespace dan punctuation\n",
        "- Subword-level: BPE, WordPiece, SentencePiece\n",
        "- Character-level: Setiap karakter adalah token\n",
        "- Hybrid approaches: Kombinasi berbagai strategi\n",
        "<br><br>\n",
        "\n",
        "###c. Vocabulary Management:\n",
        "\n",
        "- Vocabulary size: Trade-off antara coverage dan efficiency\n",
        "- OOV (Out-of-Vocabulary) handling: Unknown tokens, subword fallback\n",
        "- Frequency thresholding: Menghilangkan token yang jarang\n",
        "- Special tokens: <START>, <END>, <UNK>, <PAD>\n",
        "<br><br>\n",
        "\n",
        "###Tokenization Challenges:\n",
        "\n",
        "- Ambiguous boundaries: Contractions, hyphenated words\n",
        "- Multilingual: Berbagai script dan writing systems\n",
        "- Domain-specific: Medical, legal, technical terminology\n",
        "- Social media: Hashtags, mentions, emojis\n",
        "- Morphologically rich languages: Agglutinative languages\n",
        "<br><br>\n",
        "\n",
        "###Subword Tokenization:\n",
        "Mengatasi masalah OOV dan vocabulary size dengan memecah kata menjadi subword units:\n",
        "<br><br>\n",
        "\n",
        "###a. Byte Pair Encoding (BPE):\n",
        "\n",
        "- Iteratively merge most frequent character pairs\n",
        "- Balances between character-level dan word-level\n",
        "- Dapat menangani morfologi dan OOV words\n",
        "<br><br>\n",
        "\n",
        "###b. WordPiece:\n",
        "\n",
        "- Similar dengan BPE tetapi menggunakan likelihood-based merging\n",
        "- Digunakan dalam BERT dan model Google lainnya\n",
        "- Optimizes for language model perplexity\n",
        "<br><br>\n",
        "\n",
        "###c. SentencePiece:\n",
        "\n",
        "- Treats text as sequence of Unicode characters\n",
        "- Tidak memerlukan pre-tokenization\n",
        "- Language-agnostic approach\n",
        "<br><br>\n",
        "\n",
        "###Sequence Preparation:\n",
        "\n",
        "- Padding: Menyamakan panjang sequence dalam batch\n",
        "- Truncation: Memotong sequence yang terlalu panjang\n",
        "- Attention masks: Menandai posisi yang valid vs padding\n",
        "- Special token insertion: Menambahkan token khusus untuk tugas tertentu"
      ],
      "metadata": {
        "id": "p7YWlyTcH_NL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Advanced Text Preprocessing"
      ],
      "metadata": {
        "id": "UKeyIgwjIAb_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import unicodedata\n",
        "\n",
        "def preprocess_sentence(w):\n",
        "    # Convert to lowercase\n",
        "    w = w.lower().strip()\n",
        "\n",
        "    # Creating a space between a word and the punctuation\n",
        "    w = re.sub(r\"([?.!,¿])\", r\" \\1 \", w)\n",
        "    w = re.sub(r'[\" \"]+', \" \", w)\n",
        "\n",
        "    # Replacing everything with space except (a-z, A-Z, \".\", \"?\", \"!\", \",\")\n",
        "    w = re.sub(r\"[^a-zA-Z?.!,¿]+\", \" \", w)\n",
        "\n",
        "    w = w.strip()\n",
        "\n",
        "    # Adding start and end tokens\n",
        "    w = '<start> ' + w + ' <end>'\n",
        "    return w\n",
        "\n",
        "def create_dataset(path, num_examples=None):\n",
        "    lines = open(path, encoding='UTF-8').read().strip().split('\\n')\n",
        "\n",
        "    word_pairs = [[preprocess_sentence(w) for w in l.split('\\t')]\n",
        "                  for l in lines[:num_examples]]\n",
        "\n",
        "    return zip(*word_pairs)\n",
        "\n",
        "# Tokenization\n",
        "def tokenize(lang):\n",
        "    lang_tokenizer = tf.keras.preprocessing.text.Tokenizer(filters='')\n",
        "    lang_tokenizer.fit_on_texts(lang)\n",
        "\n",
        "    tensor = lang_tokenizer.texts_to_sequences(lang)\n",
        "    tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor, padding='post')\n",
        "\n",
        "    return tensor, lang_tokenizer\n",
        "\n",
        "def load_dataset(path, num_examples=None):\n",
        "    # Create dataset\n",
        "    target_lang, input_lang = create_dataset(path, num_examples)\n",
        "\n",
        "    input_tensor, input_lang_tokenizer = tokenize(input_lang)\n",
        "    target_tensor, target_lang_tokenizer = tokenize(target_lang)\n",
        "\n",
        "    return input_tensor, target_tensor, input_lang_tokenizer, target_lang_tokenizer"
      ],
      "metadata": {
        "id": "Pm_ZBeH2IBj-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##7. Evaluasi Model NLP\n",
        "###Teori Evaluasi NLP\n",
        "Evaluasi model NLP berbeda dengan evaluasi model machine learning lainnya karena output berupa text yang memiliki karakteristik khusus. Evaluasi yang baik harus mempertimbangkan berbagai aspek seperti fluency, adequacy, dan semantic similarity.\n",
        "<br><br>\n",
        "\n",
        "###Tantangan dalam Evaluasi NLP:\n",
        "\n",
        "- Subjektivity: Kualitas text sering subjektif\n",
        "- Multiple correct answers: Satu input dapat memiliki berbagai output yang benar\n",
        "- Context dependency: Kualitas bergantung pada konteks penggunaan\n",
        "- Semantic vs syntactic: Evaluasi makna vs struktur\n",
        "- Human evaluation cost: Evaluasi manusia mahal dan tidak scalable\n",
        "<br><br>\n",
        "\n",
        "###Jenis-jenis Metrik Evaluasi:\n",
        "\n",
        "###a. Reference-based Metrics:\n",
        "Memerlukan ground truth reference untuk comparison\n",
        "###BLEU (Bilingual Evaluation Understudy):\n",
        "\n",
        "- Mengukur n-gram precision antara candidate dan reference\n",
        "- BLEU = BP × exp(Σ w_n log p_n)\n",
        "- BP (Brevity Penalty) menghukum output yang terlalu pendek\n",
        "- Baik untuk machine translation, kurang untuk generation tasks\n",
        "- Kelemahan: tidak mempertimbangkan semantic similarity\n",
        "<br><br>\n",
        "\n",
        "###ROUGE (Recall-Oriented Understudy for Gisting Evaluation):\n",
        "\n",
        "- Focus pada recall rather than precision\n",
        "- ROUGE-N: n-gram recall\n",
        "- ROUGE-L: Longest Common Subsequence\n",
        "- ROUGE-W: Weighted LCS\n",
        "- Cocok untuk summarization tasks\n",
        "<br><br>\n",
        "\n",
        "###METEOR (Metric for Evaluation of Translation with Explicit ORdering):\n",
        "\n",
        "- Combines precision, recall, dan alignment\n",
        "- Mempertimbangkan synonyms dan stemming\n",
        "- Lebih korelasi dengan human judgment dibanding BLEU\n",
        "<br><br>\n",
        "\n",
        "###b. Embedding-based Metrics:\n",
        "Menggunakan word/sentence embeddings untuk semantic comparison\n",
        "###BERTScore:\n",
        "\n",
        "- Menggunakan BERT embeddings untuk token-level similarity\n",
        "- Menghitung precision, recall, dan F1 pada embedding space\n",
        "- Lebih sensitive terhadap semantic meaning\n",
        "<br><br>\n",
        "\n",
        "###Sentence-BERT:\n",
        "\n",
        "- Menggunakan sentence-level embeddings\n",
        "- Cosine similarity antara candidate dan reference embeddings\n",
        "<br><br>\n",
        "\n",
        "###c. Task-specific Metrics:\n",
        "\n",
        "- Sentiment Analysis: Accuracy, Precision, Recall, F1\n",
        "- Named Entity Recognition: Entity-level F1, exact match\n",
        "- Question Answering: Exact Match, F1 on tokens\n",
        "- Text Classification: Standard classification metrics\n",
        "<br><br>\n",
        "\n",
        "\n",
        "###Human Evaluation:\n",
        "\n",
        "- Fluency: Seberapa natural dan grammatical text yang dihasilkan\n",
        "- Adequacy: Seberapa baik meaning dari source dipertahankan\n",
        "- Relevance: Seberapa relevan output dengan input/context\n",
        "- Coherence: Konsistensi internal dalam text\n",
        "- Inter-annotator agreement: Konsistensi antar evaluator\n",
        "<br><br>\n",
        "\n",
        "###Evaluation Best Practices:\n",
        "\n",
        "- Multiple metrics: Gunakan berbagai metrik untuk comprehensive evaluation\n",
        "- Statistical significance: Test significance dari improvement\n",
        "- Error analysis: Analisis kualitatif dari errors\n",
        "- Domain evaluation: Test pada berbagai domain\n",
        "- Robustness testing: Evaluation pada adversarial examples\n",
        "<br><br>\n",
        "\n",
        "###Challenges dengan Reference-based Metrics:\n",
        "\n",
        "- Single reference limitation: Real-world tasks sering memiliki multiple valid outputs\n",
        "- Surface-level comparison: Metrics seperti BLEU hanya compare surface forms\n",
        "- Gaming metrics: Model dapat dioptimasi untuk metric tanpa improving actual quality\n",
        "- Context ignorance: Tidak mempertimbangkan context yang lebih luas"
      ],
      "metadata": {
        "id": "K3pwljS4IDRN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###BLEU Score untuk Machine Translation"
      ],
      "metadata": {
        "id": "vQEJ8YmtIEmp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.translate.bleu_score import sentence_bleu, corpus_bleu\n",
        "\n",
        "def evaluate_translation(encoder, decoder, sentence, input_lang_tokenizer, target_lang_tokenizer, max_length):\n",
        "    attention_plot = np.zeros((max_length, max_length))\n",
        "\n",
        "    sentence = preprocess_sentence(sentence)\n",
        "    inputs = [input_lang_tokenizer.word_index[i] for i in sentence.split(' ')]\n",
        "    inputs = tf.keras.preprocessing.sequence.pad_sequences([inputs], maxlen=max_length, padding='post')\n",
        "    inputs = tf.convert_to_tensor(inputs)\n",
        "\n",
        "    result = ''\n",
        "\n",
        "    hidden = [tf.zeros((1, units))]\n",
        "    enc_out, enc_hidden = encoder(inputs, hidden)\n",
        "\n",
        "    dec_hidden = enc_hidden\n",
        "    dec_input = tf.expand_dims([target_lang_tokenizer.word_index['<start>']], 0)\n",
        "\n",
        "    for t in range(max_length):\n",
        "        predictions, dec_hidden, attention_weights = decoder(dec_input, dec_hidden, enc_out)\n",
        "\n",
        "        attention_weights = tf.reshape(attention_weights, (-1, ))\n",
        "        attention_plot[t] = attention_weights.numpy()\n",
        "\n",
        "        predicted_id = tf.argmax(predictions[0]).numpy()\n",
        "\n",
        "        result += target_lang_tokenizer.index_word[predicted_id] + ' '\n",
        "\n",
        "        if target_lang_tokenizer.index_word[predicted_id] == '<end>':\n",
        "            return result, sentence, attention_plot\n",
        "\n",
        "        dec_input = tf.expand_dims([predicted_id], 0)\n",
        "\n",
        "    return result, sentence, attention_plot\n",
        "\n",
        "# Calculate BLEU score\n",
        "def calculate_bleu_score(references, candidates):\n",
        "    \"\"\"\n",
        "    Calculate BLEU score for a set of reference and candidate sentences\n",
        "    \"\"\"\n",
        "    bleu_score = corpus_bleu(references, candidates)\n",
        "    return bleu_score"
      ],
      "metadata": {
        "id": "XjurVOZPIGrq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Kesimpulan\n",
        "Chapter 16 membahas aplikasi RNN dan Attention untuk NLP:\n",
        "\n",
        "- Character-Level Language Models: Untuk text generation sederhana\n",
        "- Sentiment Analysis: Menggunakan LSTM untuk klasifikasi teks\n",
        "- Neural Machine Translation: Encoder-decoder dengan attention\n",
        "- Attention Mechanism: Meningkatkan performa pada sekuens panjang\n",
        "- Text Preprocessing: Teknik preprocessing yang robust\n",
        "- Evaluation Metrics: BLEU score dan metrik evaluasi NLP lainnya\n",
        "<br><br>\n",
        "\n",
        "###Key Takeaways:\n",
        "\n",
        "- RNN cocok untuk tugas NLP yang membutuhkan pemahaman konteks\n",
        "- Attention mechanism sangat penting untuk tugas seq2seq\n",
        "- Preprocessing yang baik sangat kritikal untuk performa model\n",
        "- Evaluasi model NLP membutuhkan metrik khusus seperti BLEU\n",
        "- Transformer (preview) menunjukkan arah masa depan NLP\n",
        "<Br><br>\n",
        "\n",
        "###Tips Praktis:\n",
        "\n",
        "- Gunakan teacher forcing untuk training yang lebih stabil\n",
        "- Implementasikan attention untuk tugas translation\n",
        "- Lakukan preprocessing yang konsisten antara training dan inference\n",
        "- Monitor attention weights untuk interpretability\n",
        "- Pertimbangkan model pre-trained untuk tugas praktis"
      ],
      "metadata": {
        "id": "2g8B3EfpIIwC"
      }
    }
  ]
}