{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP+c+UnWiTdtMf7VHOqJ3rO",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bintangnabiil/Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow/blob/main/Rangkuman_Chapter_17.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Chapter 17: Representation Learning dan Generative Learning menggunakan Autoencoders\n",
        "Autoencoder adalah jenis neural network yang dilatih untuk merekonstruksi inputnya. Struktur dasar autoencoder terdiri dari dua bagian utama: encoder yang mengkompresi input menjadi representasi laten (latent representation), dan decoder yang merekonstruksi output dari representasi laten tersebut. Autoencoder memiliki banyak aplikasi praktis dalam machine learning, mulai dari dimensionality reduction, feature learning, hingga generative modeling.\n",
        "<br><br>\n",
        "\n",
        "##Arsitektur Autoencoder\n",
        "Autoencoder memiliki arsitektur yang simetris dengan bottleneck di tengah:\n",
        "\n",
        "- Input Layer: Menerima data original\n",
        "- Encoder: Mengkompresi input menjadi representasi laten yang lebih kecil\n",
        "- Latent Space (Bottleneck): Representasi terkompresi dari input\n",
        "- Decoder: Merekonstruksi data dari representasi laten\n",
        "- Output Layer: Menghasilkan rekonstruksi yang seharusnya mirip dengan input\n",
        "<br><br>\n",
        "\n",
        "##Fungsi Loss\n",
        "Autoencoder dilatih dengan meminimalkan reconstruction loss, yaitu perbedaan antara input original dan output yang direkonstruksi. Untuk data kontinu, biasanya menggunakan Mean Squared Error (MSE), sedangkan untuk data binary menggunakan binary cross-entropy."
      ],
      "metadata": {
        "id": "_eV9u9Yafhvo"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i0VuB5g3fdLx"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Contoh implementasi autoencoder sederhana\n",
        "def create_simple_autoencoder(input_dim, encoding_dim):\n",
        "    # Encoder\n",
        "    input_layer = keras.layers.Input(shape=(input_dim,))\n",
        "    encoded = keras.layers.Dense(encoding_dim, activation='relu')(input_layer)\n",
        "\n",
        "    # Decoder\n",
        "    decoded = keras.layers.Dense(input_dim, activation='sigmoid')(encoded)\n",
        "\n",
        "    # Autoencoder model\n",
        "    autoencoder = keras.Model(input_layer, decoded)\n",
        "\n",
        "    # Encoder model (untuk mendapatkan representasi laten)\n",
        "    encoder = keras.Model(input_layer, encoded)\n",
        "\n",
        "    # Decoder model\n",
        "    encoded_input = keras.layers.Input(shape=(encoding_dim,))\n",
        "    decoder_layer = autoencoder.layers[-1]\n",
        "    decoder = keras.Model(encoded_input, decoder_layer(encoded_input))\n",
        "\n",
        "    return autoencoder, encoder, decoder\n",
        "\n",
        "# Compile model\n",
        "autoencoder, encoder, decoder = create_simple_autoencoder(784, 32)\n",
        "autoencoder.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Jenis-jenis Autoencoder\n",
        "###1. Undercomplete Autoencoder\n",
        "Undercomplete autoencoder memiliki dimensi latent space yang lebih kecil dari input. Ini memaksa model untuk belajar representasi yang paling penting dari data, sehingga berfungsi sebagai dimensionality reduction."
      ],
      "metadata": {
        "id": "9faFMa_ff5QN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Implementasi Undercomplete Autoencoder untuk MNIST\n",
        "from tensorflow.keras.datasets import mnist\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "# Load dan preprocess data MNIST\n",
        "(x_train, _), (x_test, _) = mnist.load_data()\n",
        "x_train = x_train.astype('float32') / 255.0\n",
        "x_test = x_test.astype('float32') / 255.0\n",
        "x_train = x_train.reshape((len(x_train), np.prod(x_train.shape[1:])))\n",
        "x_test = x_test.reshape((len(x_test), np.prod(x_test.shape[1:])))\n",
        "\n",
        "# Buat undercomplete autoencoder\n",
        "input_dim = 784  # 28x28 pixels\n",
        "encoding_dims = [128, 64, 32]  # Progressively smaller\n",
        "\n",
        "def create_deep_autoencoder(input_dim, encoding_dims):\n",
        "    input_layer = keras.layers.Input(shape=(input_dim,))\n",
        "\n",
        "    # Encoder\n",
        "    x = input_layer\n",
        "    for dim in encoding_dims:\n",
        "        x = keras.layers.Dense(dim, activation='relu')(x)\n",
        "\n",
        "    encoded = x\n",
        "\n",
        "    # Decoder\n",
        "    for dim in reversed(encoding_dims[:-1]):\n",
        "        x = keras.layers.Dense(dim, activation='relu')(x)\n",
        "\n",
        "    decoded = keras.layers.Dense(input_dim, activation='sigmoid')(x)\n",
        "\n",
        "    autoencoder = keras.Model(input_layer, decoded)\n",
        "    encoder = keras.Model(input_layer, encoded)\n",
        "\n",
        "    return autoencoder, encoder\n",
        "\n",
        "autoencoder, encoder = create_deep_autoencoder(784, [128, 64, 32])\n",
        "autoencoder.compile(optimizer='adam', loss='binary_crossentropy')\n",
        "\n",
        "# Training\n",
        "history = autoencoder.fit(x_train, x_train,\n",
        "                         epochs=50,\n",
        "                         batch_size=256,\n",
        "                         shuffle=True,\n",
        "                         validation_data=(x_test, x_test))"
      ],
      "metadata": {
        "id": "x8sMcJh6f-xD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##2. Overcomplete Autoencoder dengan Regularization\n",
        "Overcomplete autoencoder memiliki latent space yang lebih besar dari input. Tanpa regularization, model ini akan belajar identity function. Oleh karena itu, perlu ditambahkan regularization seperti sparse regularization atau denoising."
      ],
      "metadata": {
        "id": "Im60s5yRgAYK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Sparse Autoencoder dengan L1 regularization\n",
        "def create_sparse_autoencoder(input_dim, encoding_dim, sparsity_constraint=1e-5):\n",
        "    input_layer = keras.layers.Input(shape=(input_dim,))\n",
        "\n",
        "    # Encoder dengan L1 regularization\n",
        "    encoded = keras.layers.Dense(encoding_dim,\n",
        "                                activation='relu',\n",
        "                                activity_regularizer=keras.regularizers.l1(sparsity_constraint))(input_layer)\n",
        "\n",
        "    # Decoder\n",
        "    decoded = keras.layers.Dense(input_dim, activation='sigmoid')(encoded)\n",
        "\n",
        "    autoencoder = keras.Model(input_layer, decoded)\n",
        "    return autoencoder\n",
        "\n",
        "sparse_autoencoder = create_sparse_autoencoder(784, 1000)\n",
        "sparse_autoencoder.compile(optimizer='adam', loss='binary_crossentropy')"
      ],
      "metadata": {
        "id": "XYQ8U7XmgCBw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##3. Denoising Autoencoder\n",
        "Denoising autoencoder dilatih untuk merekonstruksi input yang bersih dari input yang telah ditambahkan noise. Ini membantu model belajar representasi yang lebih robust dan berguna."
      ],
      "metadata": {
        "id": "Edwkq4XOgDuC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Implementasi Denoising Autoencoder\n",
        "def add_noise(data, noise_factor=0.5):\n",
        "    \"\"\"Menambahkan Gaussian noise ke data\"\"\"\n",
        "    noise = np.random.normal(loc=0.0, scale=1.0, size=data.shape)\n",
        "    return np.clip(data + noise_factor * noise, 0.0, 1.0)\n",
        "\n",
        "def create_denoising_autoencoder(input_dim, encoding_dim):\n",
        "    input_layer = keras.layers.Input(shape=(input_dim,))\n",
        "\n",
        "    # Encoder\n",
        "    encoded = keras.layers.Dense(encoding_dim, activation='relu')(input_layer)\n",
        "\n",
        "    # Decoder\n",
        "    decoded = keras.layers.Dense(input_dim, activation='sigmoid')(encoded)\n",
        "\n",
        "    autoencoder = keras.Model(input_layer, decoded)\n",
        "    return autoencoder\n",
        "\n",
        "# Prepare noisy data\n",
        "x_train_noisy = add_noise(x_train, 0.5)\n",
        "x_test_noisy = add_noise(x_test, 0.5)\n",
        "\n",
        "denoising_autoencoder = create_denoising_autoencoder(784, 128)\n",
        "denoising_autoencoder.compile(optimizer='adam', loss='binary_crossentropy')\n",
        "\n",
        "# Training dengan noisy input dan clean target\n",
        "history = denoising_autoencoder.fit(x_train_noisy, x_train,\n",
        "                                   epochs=50,\n",
        "                                   batch_size=128,\n",
        "                                   validation_data=(x_test_noisy, x_test))"
      ],
      "metadata": {
        "id": "k_RsfucEgFOx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Variational Autoencoder (VAE)\n",
        "Variational Autoencoder adalah pengembangan dari autoencoder tradisional yang dapat menghasilkan data baru. VAE menggunakan pendekatan probabilistik dimana encoder menghasilkan distribusi probabilitas (mean dan variance) dari latent variables, bukan nilai deterministik.\n",
        "<br><br>\n",
        "\n",
        "##Konsep Teoritis VAE\n",
        "VAE berdasarkan pada prinsip variational inference dan memiliki komponen loss yang terdiri dari:\n",
        "\n",
        "- Reconstruction Loss: Mengukur seberapa baik decoder merekonstruksi input\n",
        "- KL Divergence Loss: Mengukur perbedaan antara distribusi laten yang dipelajari dengan prior distribution (biasanya standard normal)"
      ],
      "metadata": {
        "id": "YJY4JrGLgHNR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Implementasi Variational Autoencoder\n",
        "class VAE(keras.Model):\n",
        "    def __init__(self, latent_dim, input_dim):\n",
        "        super(VAE, self).__init__()\n",
        "        self.latent_dim = latent_dim\n",
        "        self.input_dim = input_dim\n",
        "\n",
        "        # Encoder\n",
        "        self.encoder = keras.Sequential([\n",
        "            keras.layers.InputLayer(input_shape=(input_dim,)),\n",
        "            keras.layers.Dense(512, activation='relu'),\n",
        "            keras.layers.Dense(256, activation='relu'),\n",
        "            keras.layers.Dense(latent_dim + latent_dim),  # mean dan log_var\n",
        "        ])\n",
        "\n",
        "        # Decoder\n",
        "        self.decoder = keras.Sequential([\n",
        "            keras.layers.InputLayer(input_shape=(latent_dim,)),\n",
        "            keras.layers.Dense(256, activation='relu'),\n",
        "            keras.layers.Dense(512, activation='relu'),\n",
        "            keras.layers.Dense(input_dim, activation='sigmoid')\n",
        "        ])\n",
        "\n",
        "    def encode(self, x):\n",
        "        mean_logvar = self.encoder(x)\n",
        "        mean, logvar = tf.split(mean_logvar, num_or_size_splits=2, axis=1)\n",
        "        return mean, logvar\n",
        "\n",
        "    def reparameterize(self, mean, logvar):\n",
        "        \"\"\"Reparameterization trick\"\"\"\n",
        "        eps = tf.random.normal(shape=mean.shape)\n",
        "        return eps * tf.exp(logvar * 0.5) + mean\n",
        "\n",
        "    def decode(self, z):\n",
        "        return self.decoder(z)\n",
        "\n",
        "    def call(self, x):\n",
        "        mean, logvar = self.encode(x)\n",
        "        z = self.reparameterize(mean, logvar)\n",
        "        x_recon = self.decode(z)\n",
        "        return x_recon, mean, logvar\n",
        "\n",
        "# Loss function untuk VAE\n",
        "def vae_loss(x, x_recon, mean, logvar):\n",
        "    # Reconstruction loss\n",
        "    recon_loss = keras.losses.binary_crossentropy(x, x_recon)\n",
        "    recon_loss = tf.reduce_mean(recon_loss)\n",
        "\n",
        "    # KL divergence loss\n",
        "    kl_loss = -0.5 * tf.reduce_mean(1 + logvar - tf.square(mean) - tf.exp(logvar))\n",
        "\n",
        "    total_loss = recon_loss + kl_loss\n",
        "    return total_loss, recon_loss, kl_loss\n",
        "\n",
        "# Training VAE\n",
        "vae = VAE(latent_dim=20, input_dim=784)\n",
        "optimizer = keras.optimizers.Adam(learning_rate=1e-3)\n",
        "\n",
        "@tf.function\n",
        "def train_step(x):\n",
        "    with tf.GradientTape() as tape:\n",
        "        x_recon, mean, logvar = vae(x)\n",
        "        loss, recon_loss, kl_loss = vae_loss(x, x_recon, mean, logvar)\n",
        "\n",
        "    gradients = tape.gradient(loss, vae.trainable_variables)\n",
        "    optimizer.apply_gradients(zip(gradients, vae.trainable_variables))\n",
        "\n",
        "    return loss, recon_loss, kl_loss\n",
        "\n",
        "# Training loop\n",
        "epochs = 100\n",
        "batch_size = 128\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    for i in range(0, len(x_train), batch_size):\n",
        "        batch = x_train[i:i+batch_size]\n",
        "        loss, recon_loss, kl_loss = train_step(batch)\n",
        "\n",
        "    if epoch % 10 == 0:\n",
        "        print(f'Epoch {epoch}, Loss: {loss:.4f}, Recon: {recon_loss:.4f}, KL: {kl_loss:.4f}')"
      ],
      "metadata": {
        "id": "yA96bQiSgMxR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Aplikasi Autoencoder\n",
        "###1. Dimensionality Reduction\n",
        "Autoencoder dapat digunakan sebagai alternatif non-linear untuk PCA dalam dimensionality reduction."
      ],
      "metadata": {
        "id": "8474cXx3gPWm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualisasi hasil encoding\n",
        "def visualize_latent_space(encoder, data, labels, latent_dim):\n",
        "    if latent_dim == 2:\n",
        "        # Jika latent dimension = 2, plot langsung\n",
        "        encoded_data = encoder.predict(data)\n",
        "        plt.figure(figsize=(10, 8))\n",
        "        scatter = plt.scatter(encoded_data[:, 0], encoded_data[:, 1], c=labels, cmap='tab10')\n",
        "        plt.colorbar(scatter)\n",
        "        plt.title('Latent Space Representation')\n",
        "        plt.show()\n",
        "    else:\n",
        "        # Jika latent dimension > 2, gunakan t-SNE\n",
        "        from sklearn.manifold import TSNE\n",
        "        encoded_data = encoder.predict(data)\n",
        "        tsne = TSNE(n_components=2, random_state=42)\n",
        "        encoded_2d = tsne.fit_transform(encoded_data)\n",
        "\n",
        "        plt.figure(figsize=(10, 8))\n",
        "        scatter = plt.scatter(encoded_2d[:, 0], encoded_2d[:, 1], c=labels, cmap='tab10')\n",
        "        plt.colorbar(scatter)\n",
        "        plt.title('t-SNE of Latent Space')\n",
        "        plt.show()\n",
        "\n",
        "# Contoh penggunaan untuk visualisasi\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "x_train = x_train.astype('float32') / 255.0\n",
        "x_train = x_train.reshape((len(x_train), 784))\n",
        "\n",
        "# Visualisasi latent space\n",
        "visualize_latent_space(encoder, x_train[:5000], y_train[:5000], 32)"
      ],
      "metadata": {
        "id": "uG-6jsy0gRvT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###2. Anomaly Detection\n",
        "Autoencoder dapat digunakan untuk mendeteksi anomali dengan mengukur reconstruction error."
      ],
      "metadata": {
        "id": "3dbPRYOqgTdv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Anomaly Detection menggunakan Autoencoder\n",
        "def detect_anomalies(autoencoder, data, threshold_percentile=95):\n",
        "    \"\"\"\n",
        "    Mendeteksi anomali berdasarkan reconstruction error\n",
        "    \"\"\"\n",
        "    # Prediksi rekonstruksi\n",
        "    reconstructed = autoencoder.predict(data)\n",
        "\n",
        "    # Hitung reconstruction error\n",
        "    mse = np.mean(np.power(data - reconstructed, 2), axis=1)\n",
        "\n",
        "    # Tentukan threshold berdasarkan percentile\n",
        "    threshold = np.percentile(mse, threshold_percentile)\n",
        "\n",
        "    # Identifikasi anomali\n",
        "    anomalies = mse > threshold\n",
        "\n",
        "    return anomalies, mse, threshold\n",
        "\n",
        "# Contoh penggunaan\n",
        "anomalies, errors, threshold = detect_anomalies(autoencoder, x_test)\n",
        "\n",
        "print(f\"Jumlah anomali terdeteksi: {np.sum(anomalies)}\")\n",
        "print(f\"Threshold: {threshold:.4f}\")\n",
        "\n",
        "# Visualisasi hasil\n",
        "plt.figure(figsize=(12, 4))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.hist(errors, bins=50, alpha=0.7)\n",
        "plt.axvline(threshold, color='red', linestyle='--', label=f'Threshold ({threshold:.4f})')\n",
        "plt.xlabel('Reconstruction Error')\n",
        "plt.ylabel('Frequency')\n",
        "plt.title('Distribution of Reconstruction Errors')\n",
        "plt.legend()\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.scatter(range(len(errors)), errors, c=anomalies, cmap='coolwarm', alpha=0.6)\n",
        "plt.axhline(threshold, color='red', linestyle='--')\n",
        "plt.xlabel('Sample Index')\n",
        "plt.ylabel('Reconstruction Error')\n",
        "plt.title('Anomaly Detection Results')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "gcdz-corgUjU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###3. Data Generation dengan VAE\n",
        "VAE dapat menghasilkan data baru dengan sampling dari latent space."
      ],
      "metadata": {
        "id": "_GuAcm0-gXWC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate new data menggunakan VAE\n",
        "def generate_new_samples(vae, num_samples=10):\n",
        "    \"\"\"Generate new samples from VAE\"\"\"\n",
        "    # Sample dari standard normal distribution\n",
        "    z = tf.random.normal(shape=(num_samples, vae.latent_dim))\n",
        "\n",
        "    # Decode untuk mendapatkan generated samples\n",
        "    generated_samples = vae.decode(z)\n",
        "\n",
        "    return generated_samples.numpy()\n",
        "\n",
        "# Generate dan visualisasi new samples\n",
        "new_samples = generate_new_samples(vae, 25)\n",
        "\n",
        "# Reshape untuk visualisasi (28x28 untuk MNIST)\n",
        "new_samples = new_samples.reshape(-1, 28, 28)\n",
        "\n",
        "# Plot generated samples\n",
        "plt.figure(figsize=(10, 10))\n",
        "for i in range(25):\n",
        "    plt.subplot(5, 5, i+1)\n",
        "    plt.imshow(new_samples[i], cmap='gray')\n",
        "    plt.axis('off')\n",
        "plt.suptitle('Generated Samples from VAE')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "FrP7lLXPgWYD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Convolutional Autoencoder\n",
        "Untuk data image, Convolutional Autoencoder lebih efektif karena dapat mempertahankan spatial information."
      ],
      "metadata": {
        "id": "gHsc7w2egaS5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Implementasi Convolutional Autoencoder\n",
        "def create_conv_autoencoder(input_shape):\n",
        "    # Encoder\n",
        "    input_layer = keras.layers.Input(shape=input_shape)\n",
        "\n",
        "    # Encoder layers\n",
        "    x = keras.layers.Conv2D(32, (3, 3), activation='relu', padding='same')(input_layer)\n",
        "    x = keras.layers.MaxPooling2D((2, 2), padding='same')(x)\n",
        "    x = keras.layers.Conv2D(16, (3, 3), activation='relu', padding='same')(x)\n",
        "    x = keras.layers.MaxPooling2D((2, 2), padding='same')(x)\n",
        "    x = keras.layers.Conv2D(8, (3, 3), activation='relu', padding='same')(x)\n",
        "    encoded = keras.layers.MaxPooling2D((2, 2), padding='same')(x)\n",
        "\n",
        "    # Decoder layers\n",
        "    x = keras.layers.Conv2D(8, (3, 3), activation='relu', padding='same')(encoded)\n",
        "    x = keras.layers.UpSampling2D((2, 2))(x)\n",
        "    x = keras.layers.Conv2D(16, (3, 3), activation='relu', padding='same')(x)\n",
        "    x = keras.layers.UpSampling2D((2, 2))(x)\n",
        "    x = keras.layers.Conv2D(32, (3, 3), activation='relu', padding='same')(x)\n",
        "    x = keras.layers.UpSampling2D((2, 2))(x)\n",
        "    decoded = keras.layers.Conv2D(1, (3, 3), activation='sigmoid', padding='same')(x)\n",
        "\n",
        "    autoencoder = keras.Model(input_layer, decoded)\n",
        "    encoder = keras.Model(input_layer, encoded)\n",
        "\n",
        "    return autoencoder, encoder\n",
        "\n",
        "# Prepare data untuk convolutional autoencoder\n",
        "(x_train, _), (x_test, _) = mnist.load_data()\n",
        "x_train = x_train.astype('float32') / 255.0\n",
        "x_test = x_test.astype('float32') / 255.0\n",
        "x_train = np.reshape(x_train, (len(x_train), 28, 28, 1))\n",
        "x_test = np.reshape(x_test, (len(x_test), 28, 28, 1))\n",
        "\n",
        "# Create dan compile model\n",
        "conv_autoencoder, conv_encoder = create_conv_autoencoder((28, 28, 1))\n",
        "conv_autoencoder.compile(optimizer='adam', loss='binary_crossentropy')\n",
        "\n",
        "# Training\n",
        "history = conv_autoencoder.fit(x_train, x_train,\n",
        "                              epochs=50,\n",
        "                              batch_size=128,\n",
        "                              validation_data=(x_test, x_test))"
      ],
      "metadata": {
        "id": "QRGmPV10gcZJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Tips dan Best Practices\n",
        "###1. Hyperparameter Tuning\n"
      ],
      "metadata": {
        "id": "ve5xCbI-geGX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Contoh hyperparameter tuning untuk autoencoder\n",
        "def tune_autoencoder_hyperparams():\n",
        "    # Parameter yang bisa di-tune\n",
        "    encoding_dims = [16, 32, 64, 128]\n",
        "    learning_rates = [0.001, 0.01, 0.1]\n",
        "    batch_sizes = [64, 128, 256]\n",
        "\n",
        "    best_loss = float('inf')\n",
        "    best_params = {}\n",
        "\n",
        "    for encoding_dim in encoding_dims:\n",
        "        for lr in learning_rates:\n",
        "            for batch_size in batch_sizes:\n",
        "                # Create model\n",
        "                autoencoder, _ = create_simple_autoencoder(784, encoding_dim)\n",
        "                autoencoder.compile(optimizer=keras.optimizers.Adam(learning_rate=lr),\n",
        "                                  loss='binary_crossentropy')\n",
        "\n",
        "                # Train dengan early stopping\n",
        "                early_stopping = keras.callbacks.EarlyStopping(monitor='val_loss',\n",
        "                                                              patience=5,\n",
        "                                                              restore_best_weights=True)\n",
        "\n",
        "                history = autoencoder.fit(x_train, x_train,\n",
        "                                        epochs=20,\n",
        "                                        batch_size=batch_size,\n",
        "                                        validation_data=(x_test, x_test),\n",
        "                                        callbacks=[early_stopping],\n",
        "                                        verbose=0)\n",
        "\n",
        "                # Evaluasi\n",
        "                val_loss = min(history.history['val_loss'])\n",
        "\n",
        "                if val_loss < best_loss:\n",
        "                    best_loss = val_loss\n",
        "                    best_params = {\n",
        "                        'encoding_dim': encoding_dim,\n",
        "                        'learning_rate': lr,\n",
        "                        'batch_size': batch_size,\n",
        "                        'val_loss': val_loss\n",
        "                    }\n",
        "\n",
        "    return best_params\n",
        "\n",
        "# Jalankan hyperparameter tuning\n",
        "# best_params = tune_autoencoder_hyperparams()\n",
        "# print(\"Best parameters:\", best_params)"
      ],
      "metadata": {
        "id": "BZ65GVRUghbx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###2. Monitoring Training"
      ],
      "metadata": {
        "id": "wRhqnTHxgiIY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Callback untuk monitoring training\n",
        "class AutoencoderMonitor(keras.callbacks.Callback):\n",
        "    def __init__(self, validation_data, encoder, decoder):\n",
        "        self.validation_data = validation_data\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        if epoch % 10 == 0:\n",
        "            # Visualisasi rekonstruksi setiap 10 epoch\n",
        "            x_test_sample = self.validation_data[0][:5]\n",
        "            encoded_imgs = self.encoder.predict(x_test_sample)\n",
        "            decoded_imgs = self.decoder.predict(encoded_imgs)\n",
        "\n",
        "            plt.figure(figsize=(15, 4))\n",
        "            for i in range(5):\n",
        "                # Original\n",
        "                plt.subplot(2, 5, i + 1)\n",
        "                plt.imshow(x_test_sample[i].reshape(28, 28), cmap='gray')\n",
        "                plt.title(f'Original {i+1}')\n",
        "                plt.axis('off')\n",
        "\n",
        "                # Reconstructed\n",
        "                plt.subplot(2, 5, i + 1 + 5)\n",
        "                plt.imshow(decoded_imgs[i].reshape(28, 28), cmap='gray')\n",
        "                plt.title(f'Reconstructed {i+1}')\n",
        "                plt.axis('off')\n",
        "\n",
        "            plt.suptitle(f'Epoch {epoch}')\n",
        "            plt.show()\n",
        "\n",
        "# Penggunaan monitor\n",
        "monitor = AutoencoderMonitor((x_test, x_test), encoder, decoder)"
      ],
      "metadata": {
        "id": "N6kL03z-gkIO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Kesimpulan\n",
        "Autoencoder adalah tool yang sangat powerful dalam machine learning dengan berbagai aplikasi praktis. Konsep-konsep utama yang perlu dipahami:\n",
        "\n",
        "- Architecture: Encoder-decoder structure dengan bottleneck\n",
        "- Loss Function: Reconstruction loss sebagai objektif utama\n",
        "- Variants: Undercomplete, overcomplete, sparse, denoising, dan variational\n",
        "- Applications: Dimensionality reduction, feature learning, anomaly detection, dan data generation\n",
        "<br><br>\n",
        "\n",
        "Pemahaman yang mendalam tentang autoencoder memungkinkan kita untuk:\n",
        "\n",
        "- Melakukan unsupervised learning yang efektif\n",
        "- Mengekstrak representasi berguna dari data\n",
        "- Mendeteksi anomali dalam data\n",
        "- Menghasilkan data sintesis yang realistis\n",
        "\n",
        "Implementasi yang tepat memerlukan pemahaman tentang arsitektur yang sesuai dengan jenis data, tuning hyperparameter yang cermat, dan evaluasi yang komprehensif terhadap kualitas representasi yang dihasilkan."
      ],
      "metadata": {
        "id": "qfZfh3lSglfe"
      }
    }
  ]
}